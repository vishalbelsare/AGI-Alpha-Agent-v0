###############################################################################
#  docker-compose.macro.yml  â€“  Macro-Sentinel Demo  (Alpha-Factory v1 ğŸ‘ï¸âœ¨)
#
#  Hardened, regulator-friendly stack that works **online or fully offline**.
#  Inspired by best practices in the OpenAI Agents SDK guide (2025-04) and the
#  agentic-trading reference architecture.
#
#  Key features
#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  â€¢ LLM toggle  â€“ GPT-4o via OPENAI_API_KEY or Mixtral via Ollama container
#  â€¢ Time-series DB  â€“ TimescaleDB for macro ticks & VaR trace
#  â€¢ Event bus       â€“ Redis Streams for pub/sub between agents
#  â€¢ Risk cache      â€“ Qdrant vector store (optional) for similarity RAG
#  â€¢ Metrics         â€“ Prometheus + Grafana dashboards (port 3001)
#  â€¢ Health checks   â€“ every service exposes /healthz or container HC
#  â€¢ GPU optional    â€“ set ENABLE_CUDA=1 to build CUDA stage
#
#  Bring-up
#    docker compose -f docker-compose.macro.yml up -d --build
###############################################################################

version: "3.9"

x-common-env: &common-env
  TZ: UTC
  PYTHONUNBUFFERED: "1"
  OPENAI_API_KEY: "${OPENAI_API_KEY:-}"
  MODEL_NAME: "${MODEL_NAME:-gpt-4o-mini}"
  TEMPERATURE: "${TEMPERATURE:-0.2}"
  LIVE_FEED: "${LIVE_FEED:-0}"

x-health: &hc
  interval: 20s
  retries: 5

################################################################################
#  LLM fallback  â€“ runs only if no key provided
################################################################################
services:
  ollama:
    image: ollama/ollama:latest
    profiles: ["offline"]          # docker compose --profile offline â€¦
    environment:
      - OLLAMA_MODELS=mixtral:instruct
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/.well-known/ready || exit 1"]
      <<: *hc
    restart: on-failure

################################################################################
#  Core infra  â€“ TimescaleDB, Redis Streams, Qdrant (vector memory)
################################################################################
  timescaledb:
    image: timescale/timescaledb-postgis:2.15.0-pg14
    environment:
      POSTGRES_PASSWORD: "${PG_PASSWORD:-alpha}"
      POSTGRES_USER: alpha
      POSTGRES_DB: macro_ticks
    volumes:
      - timescale_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "alpha"]
      <<: *hc
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: ["redis-server","--save","", "--appendonly","no"]
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 15s
      retries: 4
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:v1.8.1
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL","wget -qO- http://localhost:6333/ready || exit 1"]
      <<: *hc
    restart: unless-stopped

################################################################################
#  Prometheus & Grafana  â€“ observability
################################################################################
  prometheus:
    image: prom/prometheus:v2.52.0
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command: ["--config.file=/etc/prometheus/prometheus.yml"]
    ports: ["9090:9090"]
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.4.2
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=alpha
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana_datasource.yml:/etc/grafana/provisioning/datasources/ds.yaml:ro
      - ./observability/grafana_dashboard.json:/var/lib/grafana/dashboards/dashboard.json:ro
    ports: ["3001:3000"]
    depends_on:
      prometheus: {condition: service_started}
    restart: unless-stopped

################################################################################
#  Orchestrator agent  â€“ houses Macro-Sentinel brain
################################################################################
  orchestrator:
    build:
      context: ../..
      dockerfile: ./Dockerfile
      args:
        ENABLE_CUDA: "${ENABLE_CUDA:-0}"
    image: alpha_factory_orchestrator:macro
    command: python /app/demo/agent_macro_entrypoint.py
    env_file: ./config.env
    environment:
      <<: *common-env
      DATABASE_URL: "postgresql://alpha:${PG_PASSWORD:-alpha}@timescaledb:5432/macro_ticks"
      REDIS_URL: "redis://redis:6379"
      VECTOR_HOST: "qdrant:6333"
      OLLAMA_BASE_URL: "${OLLAMA_BASE_URL:-http://ollama:11434/v1}"
    volumes:
      - ./:/app/demo:ro
    ports:
      - "7864:7864"
    depends_on:
      timescaledb: {condition: service_healthy}
      redis:       {condition: service_healthy}
      qdrant:      {condition: service_healthy}
      ollama:
        condition: service_started
        required: false
    healthcheck:
      test: ["CMD-SHELL","curl -f http://localhost:7864/healthz || exit 1"]
      <<: *hc
    restart: unless-stopped

################################################################################
#  Optional data-collector sidecar  â€“ streams live Fed tweets / FRED API
################################################################################
  collector:
    image: python:3.11-slim
    environment:
      <<: *common-env
      REDIS_URL: "redis://redis:6379"
      FRED_API_KEY: "${FRED_API_KEY:-}"
      TW_BEARER_TOKEN: "${TW_BEARER_TOKEN:-}"
    volumes:
      - ./collector:/collector:ro
    command: python /collector/collector.py
    depends_on:
      redis: {condition: service_healthy}
    profiles: ["live-feed"]
    restart: unless-stopped

volumes:
  timescale_data:
  redis_data:
  qdrant_data:
  ollama_models:
  grafana_data:
