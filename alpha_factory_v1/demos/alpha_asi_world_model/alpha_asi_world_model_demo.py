# SPDX-License-Identifier: Apache-2.0
# alpha_asi_world_model_demo.py ‚Äì Alpha‚ÄëFactory v1 üëÅ
# =============================================================================
# Production‚Äëgrade Œ±‚ÄëASI world‚Äëmodel demo.
# POET‚Äëstyle curriculum with MuZero learner (light MCTS), orchestrated by
# ‚â•5 Alpha‚ÄëFactory agents. Local‚Äëfirst design with optional cloud helpers.
# Python 3.11+, PyTorch ‚â•2.2. Runs on CPU or GPU.
#
# Quick start .................  python -m alpha_asi_world_model_demo --demo
# Docker .....................  python -m alpha_asi_world_model_demo --emit-docker
# Helm .......................  python -m alpha_asi_world_model_demo --emit-helm
# Notebook ...................  python -m alpha_asi_world_model_demo --emit-notebook
# UI ..........................  http://localhost:7860
# =============================================================================
"""Open‚Äëended world‚Äëmodel curriculum with strict defaults and enterprise
readiness. Configurable via `config.yaml` or environment variables;
integrates structured logging, safety guards and minimal ops helpers.

Key features
------------
* Config.yaml loader with automatic envvar override
* Structured logging (optional JSON mode)
* Lightweight MCTS for MuZeroTiny
* Multi‚Äëenvironment batches (`Config.env_batch`)
* Testable hooks and graceful shutdown
* External LLM calls sandboxed with explicit timeouts
* Zero mandatory cloud dependencies
"""
from __future__ import annotations

import argparse
import asyncio
import importlib
import json
import logging
import os
import random
import threading
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Callable, Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim

try:
    import yaml  # soft‚Äëdep; config file
except ImportError:
    yaml = None

# -----------------------------------------------------------------------------
# 0. Logging
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=os.getenv("ALPHA_ASI_LOGLEVEL", "INFO"),
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y‚Äë%m‚Äë%d %H:%M:%S",
)
LOG = logging.getLogger("alpha_asi_demo")

# =============================================================================
# 1.  Deterministic seed for reproducibility
# =============================================================================
_SEED = int(os.getenv("ALPHA_ASI_SEED", "42"))
random.seed(_SEED)
np.random.seed(_SEED)
torch.manual_seed(_SEED)


# =============================================================================
# 2.  Typed runtime configuration
#     Loads defaults ‚Üí overrides with config.yaml ‚Üí overrides with env vars.
# =============================================================================
@dataclass
class Config:
    env_batch: int = 2
    buffer_limit: int = 100_000
    hidden: int = 256
    lr: float = 5e-4
    train_batch: int = 256
    ui_tick: int = 100
    max_steps: int = 200_000
    mcts_simulations: int = 16
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    log_json: bool = False
    host: str = "127.0.0.1"
    port: int = 7860

    def update(self, **kw):
        for k, v in kw.items():
            if hasattr(self, k):
                setattr(self, k, v)


def _str_to_bool(v: str) -> bool:
    """Return True for truthy strings."""
    return v.lower() in {"1", "true", "yes", "on"}


def _load_cfg() -> Config:
    cfg = Config()
    # yaml config file optional
    if yaml:
        for p in (Path.cwd() / "config.yaml", Path.cwd() / "alpha_asi.yaml"):
            if p.exists():
                try:
                    cfg.update(**yaml.safe_load(p.read_text()))
                    LOG.info("Loaded config from %s", p)
                except Exception as e:
                    LOG.warning("Failed to parse %s: %s", p, e)
    # env overrides
    for k in cfg.__dict__.keys():
        env_key = "ALPHA_ASI_" + k.upper()
        if env_key in os.environ:
            val = os.environ[env_key]
            default = getattr(cfg, k)
            if isinstance(default, bool):
                val = _str_to_bool(val)
            else:
                try:
                    val = type(default)(val)
                except Exception:
                    pass
            setattr(cfg, k, val)
    return cfg


CFG = _load_cfg()


# =============================================================================
# 3.  A2A message bus (in‚Äëproc pub‚Äësub)
# =============================================================================
class A2ABus:
    _subs: Dict[str, List[Callable[[dict], None]]] = {}
    _lock = threading.Lock()

    @classmethod
    def publish(cls, topic: str, msg: dict):
        with cls._lock:
            for cb in list(cls._subs.get(topic, [])):
                try:
                    cb(msg)
                except Exception as exc:  # pragma: no cover
                    LOG.error("[A2A] handler error on %s: %s", topic, exc)

    @classmethod
    def subscribe(cls, topic: str, cb: Callable[[dict], None]):
        with cls._lock:
            cls._subs.setdefault(topic, []).append(cb)

    @classmethod
    def unsubscribe(cls, topic: str, cb: Callable[[dict], None]):
        """Remove a previously registered callback."""
        with cls._lock:
            handlers = cls._subs.get(topic)
            if not handlers:
                return
            try:
                handlers.remove(cb)
            except ValueError:
                return
            if not handlers:
                cls._subs.pop(topic, None)


class Agent:
    """Base‚Äëclass for micro‚Äëagents. Override .handle."""

    def __init__(self, name: str):
        self.name = name
        self._cb = self._on
        A2ABus.subscribe(name, self._cb)

    def _on(self, msg: dict):
        try:
            self.handle(msg)
        except Exception as exc:
            LOG.exception("[%s] crash: %s", self.name, exc)

    def emit(self, topic: str, msg: dict):
        A2ABus.publish(topic, msg)

    def close(self) -> None:
        """Unsubscribe the agent from the message bus."""
        A2ABus.unsubscribe(self.name, self._cb)

    # -----------------------------------------------------------------
    def handle(self, msg: dict):  # to be overridden
        raise NotImplementedError


# ---- dynamic loader ---------------------------------------------------------
REQUIRED = [
    "planning_agent.PlanningAgent",
    "research_agent.ResearchAgent",
    "strategy_agent.StrategyAgent",
    "market_agent.MarketAnalysisAgent",
    "codegen_agent.CodeGenAgent",
    "safety_agent.SafetyAgent",
]
MODROOT = "alpha_factory_v1.backend.agents."
AGENTS: Dict[str, Agent] = {}


def _boot(path: str) -> None:
    module_path, cls_name = (MODROOT + path).rsplit(".", 1)
    try:
        cls = getattr(importlib.import_module(module_path), cls_name)
        inst = cls()  # type: ignore[call-arg]
        name = getattr(inst, "name", getattr(inst, "NAME", cls_name))

        if not hasattr(inst, "handle") and hasattr(inst, "step"):
            step_fn = getattr(inst, "step")
            interval = getattr(inst, "CYCLE_SECONDS", 60) or 60

            class StepAdapter(Agent):
                def __init__(self) -> None:
                    super().__init__(name)
                    threading.Thread(target=self._loop, daemon=True).start()

                def handle(self, _msg: dict) -> None:  # noqa: D401
                    pass

                def _loop(self) -> None:
                    while True:
                        try:
                            res = step_fn()
                            if asyncio.iscoroutine(res):
                                asyncio.run(res)
                        except Exception as exc:  # pragma: no cover
                            LOG.debug("[Adapter:%s] step error: %s", name, exc)
                        time.sleep(max(1, interval))

            inst = StepAdapter()
        else:
            if not hasattr(inst, "name"):
                inst.name = name  # type: ignore[attr-defined]
        LOG.info("[BOOT] loaded real agent %s", name)
    except Exception as exc:
        name = cls_name

        class Stub(Agent):
            def handle(self, _msg: dict) -> None:  # noqa: D401
                LOG.debug("[Stub:%s] ‚Üê %s", cls_name, _msg)

        inst = Stub(name)
        LOG.warning("[BOOT] stubbed %s (%s)", cls_name, exc)

    AGENTS[name] = inst


for _p in REQUIRED:
    _boot(_p)
while len(AGENTS) < 5:
    idx = len(AGENTS) + 1

    class Fallback(Agent):
        def handle(self, _msg):
            LOG.debug("[Fallback%d] ‚Üê %s", idx, _msg)

    AGENTS[f"Fallback{idx}"] = Fallback(f"Fallback{idx}")


# =============================================================================
# 4.  MuZeroTiny with lightweight MCTS
# =============================================================================
class Repr(nn.Module):
    def __init__(self, input_dim: int, hidden: int):
        super().__init__()
        self.l = nn.Linear(input_dim, hidden)

    def forward(self, x):
        return torch.tanh(self.l(x))


class Dyn(nn.Module):
    def __init__(self, hidden: int, act_dim: int):
        super().__init__()
        self.r = nn.Linear(hidden + act_dim, 1)
        self.h = nn.Linear(hidden + act_dim, hidden)

    def forward(self, h, a):
        x = torch.cat([h, a], -1)
        return self.r(x), torch.tanh(self.h(x))


class Pred(nn.Module):
    def __init__(self, hidden: int, act_dim: int):
        super().__init__()
        self.v = nn.Linear(hidden, 1)
        self.p = nn.Linear(hidden, act_dim)

    def forward(self, h):
        return self.v(h), torch.log_softmax(self.p(h), -1)


class MuZeroTiny(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int):
        super().__init__()
        self.repr = Repr(obs_dim, CFG.hidden)
        self.dyn = Dyn(CFG.hidden, act_dim)
        self.pred = Pred(CFG.hidden, act_dim)

    def initial(self, obs):
        h = self.repr(obs)
        v, p = self.pred(h)
        return h, v, p

    def recurrent(self, h, a_onehot):
        r, h2 = self.dyn(h, a_onehot)
        v, p = self.pred(h2)
        return h2, r, v, p


# -------------------------------- MCTS ---------------------------------------
def mcts_policy(net: MuZeroTiny, obs: np.ndarray, simulations: int = 16) -> int:
    """Very small UCB‚Äëbased MCTS on top of MuZeroTiny."""
    act_dim = 4
    with torch.no_grad():
        h, v0, p0 = net.initial(torch.tensor(obs, device=CFG.device, dtype=torch.float32))
    N = np.zeros(act_dim)
    W = np.zeros(act_dim)
    P = p0.exp().cpu().numpy()
    for _ in range(simulations):
        a = np.argmax(P * (np.sqrt(N.sum() + 1e-8) / (1 + N)))
        a_one = F.one_hot(torch.tensor(a), num_classes=act_dim).float().to(CFG.device)
        h2, r, v, p = net.recurrent(h, a_one)
        q = (r + v).item()
        N[a] += 1
        W[a] += q
    best = int(np.argmax(W / (N + 1e-8)))
    return best


# =============================================================================
# 5.  MiniWorld + POET generator
# =============================================================================
@dataclass
class MiniWorld:
    size: int
    obstacles: List[Tuple[int, int]]
    goal: Tuple[int, int]
    agent: Tuple[int, int] = field(default=(0, 0))

    def reset(self):
        self.agent = (0, 0)
        return self._obs()

    def _clip(self, v):
        return max(0, min(self.size - 1, v))

    def step(self, act: int):
        dx, dy = [(0, 1), (1, 0), (0, -1), (-1, 0)][act % 4]
        nx, ny = self._clip(self.agent[0] + dx), self._clip(self.agent[1] + dy)
        if (nx, ny) in self.obstacles:
            nx, ny = self.agent
        self.agent = (nx, ny)
        done = self.agent == self.goal
        reward = 1.0 if done else -0.01
        return self._obs(), reward, done, {}

    def _obs(self):
        vec = np.zeros(self.size * self.size, dtype=np.float32)
        vec[self.agent[0] * self.size + self.agent[1]] = 1.0
        return vec


class POETGenerator:
    def __init__(self):
        self.pool: List[MiniWorld] = []

    def propose(self) -> MiniWorld:
        size = random.randint(5, 9)
        obstacles = {(random.randint(1, size - 2), random.randint(1, size - 2)) for _ in range(random.randint(0, size))}
        env = MiniWorld(size, list(obstacles), (size - 1, size - 1))
        self.pool.append(env)
        return env


# =============================================================================
# 6.  Learner wrapper
# =============================================================================
class Learner:
    def __init__(self, env: MiniWorld):
        self.net = MuZeroTiny(env.size**2, 4).to(CFG.device)
        self.opt = optim.Adam(self.net.parameters(), CFG.lr)
        self.buffer: List[Tuple[np.ndarray, float]] = []
        self.step_count = 0

    def act(self, obs):
        # epsilon‚Äëgreedy w/ MCTS fallback
        if random.random() < 0.1:
            return random.randint(0, 3)
        return mcts_policy(self.net, obs, CFG.mcts_simulations)

    def remember(self, obs, reward):
        self.buffer.append((obs, reward))
        if len(self.buffer) > CFG.buffer_limit:
            self.buffer.pop(0)

    def train_once(self) -> float:
        if len(self.buffer) < CFG.train_batch:
            return 0.0
        obs, rew = zip(*random.sample(self.buffer, CFG.train_batch))
        obs_t = torch.tensor(obs, device=CFG.device, dtype=torch.float32)
        rew_t = torch.tensor(rew, device=CFG.device)
        _, v, _ = self.net.initial(obs_t)
        loss = F.mse_loss(v.squeeze(), rew_t)
        self.opt.zero_grad()
        loss.backward()
        self.opt.step()
        return float(loss.item())


# =============================================================================
# 7.  Orchestrator (multi‚Äëenv batch)
# =============================================================================
class Orchestrator:
    def __init__(self):
        self.gen = POETGenerator()
        self.envs = [self.gen.propose() for _ in range(CFG.env_batch)]
        self.learners = [Learner(e) for e in self.envs]
        self.stop = False
        A2ABus.subscribe("orch", self._on_cmd)
        LOG.info("Orchestrator online with %d envs", CFG.env_batch)

    def _on_cmd(self, msg):
        if msg.get("cmd") == "new_env":
            idx = random.randrange(len(self.envs))
            self.envs[idx] = self.gen.propose()
            self.learners[idx] = Learner(self.envs[idx])
            LOG.info("Replaced env #%d", idx)
        elif msg.get("cmd") == "stop":
            self.stop = True

    # --------------------------------------------------------------
    def loop(self):
        obs = [e.reset() for e in self.envs]
        for t in range(CFG.max_steps):
            if self.stop:
                break
            for i, (env, learner) in enumerate(zip(self.envs, self.learners)):
                a = learner.act(obs[i])
                nxt, r, done, _ = env.step(a)
                learner.remember(obs[i], r)
                loss = learner.train_once()
                obs[i] = env.reset() if done else nxt
                if t % CFG.ui_tick == 0 and i == 0:
                    A2ABus.publish("ui", {"t": t, "r": r, "loss": loss})
        LOG.info("Orchestrator loop exit at t=%d", t)


# =============================================================================
# 8. Safety agent
# =============================================================================
class BasicSafetyAgent(Agent):
    def __init__(self):
        super().__init__("safety")

    def handle(self, msg):
        if "loss" in msg and (np.isnan(msg["loss"]) or msg["loss"] > 1e3):
            LOG.warning("[SAFETY] triggered ‚Äì halting learner")
            self.emit("orch", {"cmd": "stop"})


if "safety" not in AGENTS:
    BasicSafetyAgent()

# =============================================================================
# 9. Optional LLM planner
# =============================================================================
if os.getenv("OPENAI_API_KEY") and not os.getenv("NO_LLM"):
    try:
        import openai
        import concurrent.futures

        class LLMPlanner(Agent):
            def __init__(self):
                super().__init__("llm_planner")

            def _safe_call(self, prompt: str, timeout: int = 15) -> str:
                with concurrent.futures.ThreadPoolExecutor() as ex:
                    fut = ex.submit(
                        lambda: openai.ChatCompletion.create(
                            model=os.getenv("ALPHA_ASI_LLM_MODEL", "gpt-4o-mini"),
                            messages=[{"role": "user", "content": prompt}],
                            timeout=timeout,
                        )
                    )
                    return fut.result().choices[0].message.content

            def handle(self, msg):
                if "ask_plan" in msg:
                    try:
                        plan = self._safe_call(msg["ask_plan"])
                        self.emit("planning_agent", {"llm_plan": plan})
                    except Exception as e:
                        LOG.warning("LLMPlanner error: %s", e)

        LLMPlanner()
        LOG.info("LLM planner active")
    except Exception as exc:
        LOG.warning("LLM planner unavailable: %s", exc)

# =============================================================================
# üîü  FastAPI UI / REST / Web‚ÄëSocket endpoint
# =============================================================================
from fastapi import FastAPI, WebSocket  # noqa: E402
from fastapi.middleware.cors import CORSMiddleware  # noqa: E402
import uvicorn  # noqa: E402

app = FastAPI(title="Alpha‚ÄëASI World Model")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

orch: Optional[Orchestrator] = None


@app.on_event("startup")
async def _startup():
    global orch
    orch = Orchestrator()
    threading.Thread(target=orch.loop, daemon=True).start()


@app.get("/agents")
async def list_agents():
    return list(AGENTS.keys())


@app.post("/command")
async def send_cmd(cmd: Dict[str, str]):
    A2ABus.publish("orch", cmd)
    return {"ok": True}


@app.websocket("/ws")
async def ws_endpoint(sock: WebSocket):
    await sock.accept()
    q: List[dict] = []
    A2ABus.subscribe("ui", lambda m: q.append(m))
    try:
        while True:
            if q:
                await sock.send_text(json.dumps(q.pop(0)))
            await asyncio.sleep(0.1)
    except Exception:
        pass


# =============================================================================
# 11.  Dev‚Äëops helpers (Dockerfile / Helm / Notebook emitters)
# =============================================================================
DOCKERFILE = """FROM python:3.11-slim
WORKDIR /app
COPY . /app
RUN pip install --no-cache-dir fastapi uvicorn[standard] pydantic torch numpy nbformat PyYAML
EXPOSE 7860
CMD [\"python\", \"-m\", \"alpha_asi_world_model_demo\", \"--demo\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]
"""

HELM_VALUES = """replicaCount: 1
image:
  repository: alpha_asi_world_model
  tag: latest
service:
  port: 80
"""


def emit_docker(fp: Path = Path("Dockerfile")):
    fp.write_text(DOCKERFILE)
    print("Dockerfile ‚Üí", fp)


def emit_helm(dir_: Path = Path("helm_chart")):
    dir_.mkdir(exist_ok=True)
    (dir_ / "values.yaml").write_text(HELM_VALUES)
    (dir_ / "Chart.yaml").write_text("apiVersion: v2\nname: alpha-asi-demo\nversion: 0.1.0\n")
    print("Helm chart ‚Üí", dir_)


def emit_notebook(fp: Path = Path("alpha_asi_world_model_demo.ipynb")):
    import nbformat as nbf

    nb = nbf.v4.new_notebook()
    nb.cells = [
        nbf.v4.new_markdown_cell("# Œ±‚ÄëASI demo ‚Äì quickstart"),
        nbf.v4.new_code_cell("!python -m alpha_asi_world_model_demo --demo &"),
    ]
    nbf.write(nb, fp)
    print("Notebook ‚Üí", fp)


# =============================================================================
# 12.  CLI
# =============================================================================
def _main():
    p = argparse.ArgumentParser(prog="alpha_asi_world_model_demo")
    p.add_argument("--demo", action="store_true")
    p.add_argument("--emit-docker", action="store_true")
    p.add_argument("--emit-helm", action="store_true")
    p.add_argument("--emit-notebook", action="store_true")
    p.add_argument("--host", default=CFG.host)
    p.add_argument("--port", type=int, default=CFG.port)
    args = p.parse_args()
    if args.emit_docker:
        emit_docker()
    elif args.emit_helm:
        emit_helm()
    elif args.emit_notebook:
        emit_notebook()
    elif args.demo:
        uvicorn.run(
            "alpha_asi_world_model_demo:app",
            host=args.host,
            port=args.port,
            log_level="info",
        )
    else:
        p.print_help()


if __name__ == "__main__":
    _main()
