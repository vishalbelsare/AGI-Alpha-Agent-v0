<!--
 AIâ€‘GA Metaâ€‘Evolution Demo
 Alphaâ€‘Factoryâ€¯v1Â ğŸ‘ï¸âœ¨ â€” Multiâ€‘Agent **AGENTICâ€¯Î±â€‘AGI**
 Outâ€‘learn Â· Outâ€‘think Â· Outâ€‘strategise Â· Outâ€‘evolve
 Â©Â 2025â€¯MONTREAL.AIÂ Â Â MITÂ License
 -------------------------------------------------------------------------------
 Exhaustive README: quickâ€‘start, deepâ€‘dive, SOCâ€‘2 rails, CI/CD, K8s,
 observability, SBOM notice. Rendered as GitHubâ€‘flavoured Markdown.
-->


# ğŸŒŒÂ AlgorithmsÂ ThatÂ InventÂ Algorithms â€” <br>**AIâ€‘GA Metaâ€‘Evolution Demo**

> *â€œWhy handâ€‘craft intelligence when evolution can author it for you?â€* 
> â€”Â JeffÂ Clune, *AIâ€‘GAs: AIâ€‘GeneratingÂ Algorithms* (2019)Â 

A singleâ€‘command, browserâ€‘based showcase of Cluneâ€™s **Three Pillars**:

| Pillar | Demo realisation |
| :-- | :-- |
| **Metaâ€‘learning architectures** | Genome encodes a **typed list** of hidden sizes & activation; mutates via neuroâ€‘evolution |
| **Metaâ€‘learning the learning algorithms** | Runtime flag flips between *SGD* and a *fast Hebbian plasticity* innerâ€‘loop |
| **Generating learning environments** | `CurriculumEnv` selfâ€‘mutates â†’ *Line* â†’ *Zigâ€‘zag* â†’ *Gap* â†’ *Maze* |

Within **&lt;â€¯60â€¯s** youâ€™ll watch neural nets **rewrite their own blueprint** *while the world itself mutates to stay challenging*.

---

<details open>
<summary>ğŸ“‘Â Table of contentsÂ â€”Â click to jump</summary>

- [ğŸš€ Quickâ€‘start (Docker)](#-quickâ€‘start-docker)
- [ğŸ“ Run in Colab](#-run-in-colab)
- [ğŸš€ Production deployment](#-production-deployment)
- [ğŸ”‘ Online vs offline LLMs](#-online-vs-offline-llms)
- [ğŸ›  Architecture deepâ€‘dive](#-architecture-deepâ€‘dive)
- [ğŸ“ˆ Observability & metrics](#-observability--metrics)
- [ğŸ§ª Tests & CI](#-tests--ci)
- [â˜ï¸ Kubernetes deploy](#-kubernetes-deploy)
- [ğŸ›¡ SOCâ€‘2 & supplyâ€‘chain](#-socâ€‘2--supplyâ€‘chain)
- [ğŸ§© Tinker guide](#-tinker-guide)
- [ğŸ†˜ FAQ](#-faq)
- [ğŸ¤ Contributing](#-contributing)
- [âš–ï¸ License & credits](#-license--credits)
</details>

---

## ğŸš€ Quickâ€‘startÂ (Docker)

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/aiga_meta_evolution

# optional: --pull (signed image) --gpu (NVIDIA runtime)
./run_aiga_demo.sh
```

The service automatically resumes from the latest checkpoint if one exists,
so you can stop and restart the container without losing progress.

| Endpoint | URL | Purpose |
| --- | --- | --- |
| **Gradio** UI | <http://localhost:7862> | Click **EvolveÂ 5Â Generations** |
| **FastAPI** docs | <http://localhost:8000/docs> | Programmatic control |
| **Prometheus** | <http://localhost:8000/metrics> | `aiga_*` gauges & counters |

> ğŸ§Š **Cold build** â‰ˆÂ 40â€¯s (900â€¯MB). Subsequent runs are instantÂ (cache).

Minimal hostÂ reqsÂ â†’ DockerÂ 24, â‰¥Â 4â€¯GBÂ RAM, **noÂ GPU** needed.

## ğŸš€ Quickâ€‘startÂ (Python)

Prefer running natively? The service also launches directly from the
repository without Docker. This path is handy for quick experiments or
when Docker is unavailable.

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0
AUTO_INSTALL_MISSING=1 python check_env.py  # verify deps offline/online
pip install -r alpha_factory_v1/requirements.txt
# ensures `openai-agents` and friends are installed
python alpha_factory_v1/demos/aiga_meta_evolution/agent_aiga_entrypoint.py
# optional crossâ€‘platform launcher
python alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py --help
# or via module entrypoint
python -m alpha_factory_v1.demos.aiga_meta_evolution --help
```

Set `OPENAI_API_KEY` in your environment to enable cloud models. Without
it the demo falls back to the bundled offline mixtral model.

### ğŸ¤– OpenAI Agents bridge

Expose the evolver to the **OpenAI Agents SDK** runtime:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py
```

Requires the `openai-agents` package (installed via requirements).

The bridge registers an `aiga_evolver` agent exposing four tools:
`evolve` (run N generations), `best_alpha` (return the champion),
`checkpoint` (persist state), and `reset` (fresh population).
It works offline by routing to the local Mixtral server when no API key
is configured.

### ğŸ›°ï¸ Google ADK gateway

Set `ALPHA_FACTORY_ENABLE_ADK=true` to expose the same agent via a local
Google **Agent Development Kit** gateway:

```bash
ALPHA_FACTORY_ENABLE_ADK=true python openai_agents_bridge.py &
```

This publishes the tools over the **A2A protocol** so other agents can
orchestrate evolution remotely.
Set `ALPHA_FACTORY_ENABLE_ADK=1` in `config.env` to auto-start the gateway
when running `./run_aiga_demo.sh`.

## ğŸ” API authentication

Export `AUTH_BEARER_TOKEN` to require a static token on every API request. For
JWT-based auth, provide `JWT_PUBLIC_KEY` (PEM) and optional `JWT_ISSUER`.
The `/health` and `/metrics` endpoints remain public.

---
## ğŸš€ Production deployment

For step-by-step instructions on running the service in a production or workshop environment, see [PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md).


## ğŸ“ Run in Colab

| | |
| :-- | :-- |
| <a href="https://colab.research.google.com/github/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/aiga_meta_evolution/colab_aiga_meta_evolution.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="OpenÂ InÂ Colab"></a> | Launches the same dashboard with an automatic public URL. Ideal for workshops & quick demos. |

---

## ğŸ”‘ Online vs offlineÂ LLMs

| Environment variable | Effect |
| --- | --- |
| `OPENAI_API_KEY` **set** | Tools routed through **OpenAI Agents SDK** |
| **unset / empty** | Drops to **Mixtralâ€‘8x7Bâ€‘Instruct** via local Ollama sideâ€‘car â€“ *zero network egress* |

LLMs supply *commentary & analysis* only â€“ **core evolution is deterministic**.

## ğŸ” Alpha discovery stub

For a bite-size illustration of agent-driven opportunity scanning, run:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py
```

To query a specific domain once without starting the full runtime:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py \
  --domain supply-chain --once
```

The `alpha_discovery` agent exposes a single `identify_alpha` tool that asks the LLM to suggest three inefficiencies in a chosen domain. It works offline when `OPENAI_API_KEY` is unset.

## â™»ï¸ Alpha conversion stub

Turn a discovered opportunity into a short execution plan:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py --alpha "Battery arbitrage"
```

The tool outputs a threeâ€‘step JSON plan and logs it to `alpha_conversion_log.json`. When `OPENAI_API_KEY` is configured, it queries an OpenAI model; otherwise a sample plan is returned.

## ğŸ¤ Endâ€‘toâ€‘end workflow

Combine discovery and conversion into a single agent:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py
```

The `alpha_workflow` agent lists opportunities in the chosen domain, selects the
first suggestion and returns a short execution plan. When Google ADK is enabled
(via `ALPHA_FACTORY_ENABLE_ADK=1` and successful import of the ADK module), the same
agent is published over the A2A protocol for orchestration by external controllers.


---

## ğŸ›  Architecture deepâ€‘dive

```text
â”Œâ”€â”€ dockerâ€‘compose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ orchestrator (FastAPIÂ + UI) â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ollama (Mixtral fallback)  â”‚     â”‚ WebSocket
â”‚ prometheus (opt)       â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â–² REST / Ray RPC          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ MetaEvolver    checkpoint.json  â”‚ â”‚
â”‚  â”œâ”€ Ray / mp evaluation workers   â”‚ â”‚
â”‚  â””â”€ EvoNet(nn.Module) â”€â”€â”      â”‚ â”‚ obs/reward
â”‚              â–¼      â”‚ â”‚
â”‚ CurriculumEnv (Gymnasium)       â”‚â—€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **MetaEvolver** â€“ popÂ 24, tournamentâ€‘kÂ 3, elitismÂ 2, novelty bonus toggle 
* **EvoNet** â€“ arbitrary hidden layers, activation âˆˆÂ {relu,tanh,sigmoid}, optional HebbianÂ Î”W 
* **CurriculumEnv** â€“ 12â€¯Ã—â€¯12 grid, DFS solvability check, energy budget, genome autoâ€‘mutation 

---

## ğŸ“ˆ Observability & metrics

| Metric | Meaning |
| :-- | :-- |
| `aiga_avg_fitness` | Mean generation fitness |
| `aiga_best_fitness` | Elite fitness |
| `aiga_generations_total` | Counter |
| `aiga_curriculum_stage` | 0â€“3 |

Enable profile `telemetry` to autopush â†’ Prometheus â†’ Grafana. 
`docker compose --profile telemetry up`.

---

## ğŸ§ª Tests & CI

* **Coverage â‰¥â€¯90â€¯%** in <â€¯0.5â€¯s (`pytestÂ -q`) 
* GitHubÂ Actions â†’ lint â†’ test â†’ build â†’ Cosign sign 
* **SBOM** via *Syft* (SPDXÂ v3) per release

---

## â˜ï¸ Kubernetes deploy

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: aiga-demo }
spec:
 replicas: 1
 selector: { matchLabels: { app: aiga-demo } }
 template:
  metadata: { labels: { app: aiga-demo } }
  spec:
   containers:
   - name: orchestrator
    image: ghcr.io/montrealai/alpha-aiga:latest@sha256:<signed>
    ports:
    - { containerPort: 8000 }  # API
    - { containerPort: 7862 }  # UI
    readinessProbe:
     httpGet: { path: /health, port: 8000 }
    envFrom: [{ secretRef: { name: aiga-secrets } }]
```

*Helm chart* â†’ `infra/helm/aiga-demo/`.

---

## ğŸ›¡ SOCâ€‘2Â & supplyâ€‘chain

* Cosignâ€‘signed images (`cosign verify â€¦`) 
* Runs **nonâ€‘root UIDÂ 1001**, readâ€‘only code volume 
* Secrets via K8s / Docker *secrets* (never baked into layers) 
* Dependencies hashed (Poetry lock) & validated at runtime 
* SBOM exported; SLSAÂ levelÂ 2 pipeline

---

## ğŸ§© Tinker guide

| Goal | File | Hint |
| --- | --- | --- |
| Bigger populations | `meta_evolver.py` â†’ `pop_size` | Add `--profile gpu` |
| Faster novelty | `Genome.novelty_weight` | TryÂ 0.2 |
| New curriculum stage | `curriculum_env.py` | Extend `_valid_layout` |
| Swap LLM | `config.env` | Any OpenAI model id |
| Automate experimentation | FastAPI â†’ `/evolve/{n}` | Deterministic SHA checkpoint id |
| Manual reset | FastAPI â†’ `POST /reset` | Fresh population |
| Persist progress | FastAPI â†’ `POST /checkpoint` | Atomic save |

---

## ğŸ†˜ FAQ

| Symptom | Fix |
| :-- | :-- |
| â€œDocker not installedâ€ | <https://docs.docker.com/get-docker> |
| Port collision 7862 | Edit host port in compose |
| ARM Mac slow build | Enable **Rosetta** or `./run_aiga_demo.sh --pull` |
| GPU unseen | `sudo apt install nvidia-container-toolkit` & restart Docker |
| Colab URL missing | Reâ€‘run launch cell (ngrok quirk) |

---

## âš–ï¸ License & credits

*Source & assets* Â©Â 2025Â Montreal.AI, released under the **MIT License**. 
Huge thanks to:

* **JeffÂ Clune** â€“ visionary behind AIâ€‘GAs 
* **OpenAI, Anthropic, Google** â€“ openâ€‘sourcing crucial agent tooling 
* OSS maintainers â€“ you make this possible â™¥

> **Alphaâ€‘Factory** â€” forging intelligence that *invents* intelligence.
