<!--
 AIâ€‘GA Metaâ€‘Evolution Demo
 Alphaâ€‘Factoryâ€¯v1Â ğŸ‘ï¸âœ¨ â€” Multiâ€‘Agent **AGENTICâ€¯Î±â€‘AGI**
 Outâ€‘learn Â· Outâ€‘think Â· Outâ€‘strategise Â· Outâ€‘evolve
 Â©Â 2025â€¯MONTREAL.AIÂ Â Â Apacheâ€‘2.0Â License
 -------------------------------------------------------------------------------
 Exhaustive README: quickâ€‘start, deepâ€‘dive, SOCâ€‘2 rails, CI/CD, K8s,
 observability, SBOM notice. Rendered as GitHubâ€‘flavoured Markdown.
-->


# ğŸŒŒÂ AlgorithmsÂ ThatÂ InventÂ Algorithms â€” <br>**AIâ€‘GA Metaâ€‘Evolution Demo**

> *â€œWhy handâ€‘craft intelligence when evolution can author it for you?â€* 
> â€”Â JeffÂ Clune, *AIâ€‘GAs: AIâ€‘GeneratingÂ Algorithms* (2019)Â 

A singleâ€‘command, browserâ€‘based showcase of Cluneâ€™s **Three Pillars**:

| Pillar | Demo realisation |
| :-- | :-- |
| **Metaâ€‘learning architectures** | Genome encodes a **typed list** of hidden sizes & activation; mutates via neuroâ€‘evolution |
| **Metaâ€‘learning the learning algorithms** | Runtime flag flips between *SGD* and a *fast Hebbian plasticity* innerâ€‘loop |
| **Generating learning environments** | `CurriculumEnv` selfâ€‘mutates â†’ *Line* â†’ *Zigâ€‘zag* â†’ *Gap* â†’ *Maze* |

Within **&lt;â€¯60â€¯s** youâ€™ll watch neural nets **rewrite their own blueprint** *while the world itself mutates to stay challenging*.

---

## Disclaimer
This repository is a conceptual research prototype. References to "AGI" and
"superintelligence" describe aspirational goals and do not indicate the presence
of a real general intelligence. Use at your own risk.

---

<details open>
<summary>ğŸ“‘Â Table of contentsÂ â€”Â click to jump</summary>

- [ğŸš€ Quickâ€‘start (Docker)](#-quickâ€‘start-docker)
- [ğŸ“ Run in Colab](#-run-in-colab)
- [ğŸš€ Production deployment](#-production-deployment)
- [ğŸ”‘ Online vs offline LLMs](#-online-vs-offline-llms)
- [ğŸ›  Architecture deepâ€‘dive](#-architecture-deepâ€‘dive)
- [ğŸ“ˆ Observability & metrics](#-observability--metrics)
- [ğŸ§ª Tests & CI](#-tests--ci)
- [â˜ï¸ Kubernetes deploy](#-kubernetes-deploy)
- [ğŸ›¡ SOCâ€‘2 & supplyâ€‘chain](#-socâ€‘2--supplyâ€‘chain)
- [ğŸ§© Tinker guide](#-tinker-guide)
- [ğŸ†˜ FAQ](#-faq)
- [ğŸ¤ Contributing](#-contributing)
- [âš–ï¸ License & credits](#-license--credits)
</details>

---

## ğŸš€ Quickâ€‘startÂ (Docker)

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/aiga_meta_evolution

# optional: --pull (signed image) --gpu (NVIDIA runtime)
./run_aiga_demo.sh
```

The service automatically resumes from the latest checkpoint if one exists,
so you can stop and restart the container without losing progress.

| Endpoint | URL | Purpose |
| --- | --- | --- |
| **Gradio** UI | <http://localhost:7862> | Click **EvolveÂ 5Â Generations** |
| **FastAPI** docs | <http://localhost:8000/docs> | Programmatic control |
| **Prometheus** | <http://localhost:8000/metrics> | `aiga_*` gauges & counters |

> ğŸ§Š **Cold build** â‰ˆÂ 40â€¯s (900â€¯MB). Subsequent runs are instantÂ (cache).

Minimal hostÂ reqsÂ â†’ DockerÂ 24, â‰¥Â 4â€¯GBÂ RAM, **noÂ GPU** needed.

## ğŸš€ Quickâ€‘startÂ (Python)

Prefer running natively? The service also launches directly from the
repository without Docker. This path is handy for quick experiments or
when Docker is unavailable.

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0
AUTO_INSTALL_MISSING=1 python check_env.py  # verify deps offline/online
pip install -r alpha_factory_v1/requirements.txt
# ensures `openai-agents` and friends are installed
python alpha_factory_v1/demos/aiga_meta_evolution/agent_aiga_entrypoint.py
# offline machines can supply predownloaded wheels:
#   WHEELHOUSE=/path/to/wheels AUTO_INSTALL_MISSING=1 python check_env.py
# optional crossâ€‘platform launcher
python alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py --help
# or via module entrypoint
python -m alpha_factory_v1.demos.aiga_meta_evolution --help
```

Launch the **Ollama Mixtral** model in another terminal:

```bash
docker run -p 11434:11434 ollama/ollama:latest --models mixtral:instruct
```

If you bind the server to a custom host or port, set `OLLAMA_BASE_URL` so the
demo can reach it. Example:

```bash
docker run -p 12345:11434 ollama/ollama:latest --models mixtral:instruct
export OLLAMA_BASE_URL="http://localhost:12345"
```

Set `OPENAI_API_KEY` in your environment to enable cloud models. Without
it the demo falls back to the bundled offline mixtral model.

### Offline dependency setup

When working **airâ€‘gapped**, build a wheel cache in advance and tell
`check_env.py` where to find it. Set the `WHEELHOUSE` environment variable and
run the helper with `--wheelhouse <dir>` to install packages from that
directory:

```bash
WHEELHOUSE=/path/to/wheels AUTO_INSTALL_MISSING=1 \
  python check_env.py --auto-install --wheelhouse "$WHEELHOUSE"
```

See [scripts/README.md](../../scripts/README.md#offline-setup) for details on
creating the wheelhouse.

### Installing the OpenAI Agents SDK

The meta-evolution service depends on the **OpenAI Agents SDK** (or the
newer `agents` package) for all LLM access, even when running offline.
The optional bridge described below merely exposes the same tools over the
OpenAI runtime.

Install from PyPI:

```bash
pip install -U openai-agents
```

Offline, point `pip` to your wheelhouse:

```bash
pip install --no-index --find-links /path/to/wheels openai-agents
```

Some distributions ship the dependency as `agents`. The demo automatically
detects both. If you encounter `ModuleNotFoundError: openai_agents`, ensure
the package is installed in the active virtual environment.

### ğŸ¤– OpenAI Agents bridge

Expose the evolver to the **OpenAI Agents SDK** runtime:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py
```

Requires the `openai-agents` or `agents` package (already installed above).
If both are missing the script exits with an error.

The bridge registers an `aiga_evolver` agent exposing five tools:
`evolve` (run N generations), `best_alpha` (return the champion),
`checkpoint` (persist state), `reset` (fresh population), and
`history` (past fitness scores).
It works offline by routing to the local Mixtral server when no API key
is configured.

### ğŸ›°ï¸ Google ADK gateway

Set `ALPHA_FACTORY_ENABLE_ADK=true` to expose the same agent via a local
Google **Agent Development Kit** gateway:

```bash
ALPHA_FACTORY_ENABLE_ADK=true python openai_agents_bridge.py &
```

This publishes the tools over the **A2A protocol** so other agents can
orchestrate evolution remotely.
Set `ALPHA_FACTORY_ENABLE_ADK=1` in `config.env` to auto-start the gateway
when running `./run_aiga_demo.sh`.

Define `ALPHA_FACTORY_ADK_TOKEN` to require this token on every ADK request:

```env
ALPHA_FACTORY_ENABLE_ADK=1
ALPHA_FACTORY_ADK_TOKEN="my_secret_token"
```

The optional ADK gateway integrates with the OpenAI Agents SDK bridge and
underlying LLM providers as shown below.

![Bridge overview](bridge_overview.svg)

## ğŸ” API authentication

Export `AUTH_BEARER_TOKEN` to require a static token on every API request. For
JWT-based auth, provide `JWT_PUBLIC_KEY` (PEM) and optional `JWT_ISSUER`.
The `/health` and `/metrics` endpoints remain public.

---
## ğŸš€ Production deployment

For step-by-step instructions on running the service in a production or workshop environment, see [PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md).


## ğŸ“ Run in Colab

| | |
| :-- | :-- |
| <a href="https://colab.research.google.com/github/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/aiga_meta_evolution/colab_aiga_meta_evolution.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="OpenÂ InÂ Colab"></a> | Launches the same dashboard with an automatic public URL. Ideal for workshops & quick demos. |

The Colab notebook also explains how to **upload a wheelhouse archive** for offline installs. Follow that section to set `WHEELHOUSE` and run `check_env.py --auto-install --wheelhouse` when the runtime lacks internet access.

---

## ğŸ”‘ Online vs offlineÂ LLMs

| Environment variable | Effect |
| --- | --- |
| `OPENAI_API_KEY` **set** | Tools routed through **OpenAI Agents SDK** |
| **unset / empty** | Drops to **Mixtralâ€‘8x7Bâ€‘Instruct** via local Ollama sideâ€‘car â€“ *zero network egress* |

LLMs supply *commentary & analysis* only â€“ **core evolution is deterministic**.

## ğŸ” Alpha discovery stub

For a bite-size illustration of agent-driven opportunity scanning, run:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py
```

To query a specific domain once without starting the full runtime:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py \
  --domain supply-chain --once
```

The `alpha_discovery` agent exposes a single `identify_alpha` tool that asks the LLM to suggest three inefficiencies in a chosen domain. It works offline when `OPENAI_API_KEY` is unset.

## â™»ï¸ Alpha conversion stub

Turn a discovered opportunity into a short execution plan:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py --alpha "Battery arbitrage"
```

The tool outputs a threeâ€‘step JSON plan and logs it to `~/.aiga/alpha_conversion_log.json` by default. When `OPENAI_API_KEY` is configured, it queries an OpenAI model; otherwise a sample plan is returned.

## ğŸ¤ Endâ€‘toâ€‘end workflow

Combine discovery and conversion into a single agent:

```bash
python alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py
```

The `alpha_workflow` agent lists opportunities in the chosen domain, selects the
first suggestion and returns a short execution plan. When Google ADK is enabled
(via `ALPHA_FACTORY_ENABLE_ADK=1` and successful import of the ADK module), the same
agent is published over the A2A protocol for orchestration by external controllers.


---

## ğŸ›  Architecture deepâ€‘dive

```text
â”Œâ”€â”€ dockerâ€‘compose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ orchestrator (FastAPIÂ + UI) â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ollama (Mixtral fallback)  â”‚     â”‚ WebSocket
â”‚ prometheus (opt)       â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â–² REST / Ray RPC          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ MetaEvolver    checkpoint.json  â”‚ â”‚
â”‚  â”œâ”€ Ray / mp evaluation workers   â”‚ â”‚
â”‚  â””â”€ EvoNet(nn.Module) â”€â”€â”      â”‚ â”‚ obs/reward
â”‚              â–¼      â”‚ â”‚
â”‚ CurriculumEnv (Gymnasium)       â”‚â—€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **MetaEvolver** â€“ popÂ 24, tournamentâ€‘kÂ 3, elitismÂ 2, novelty bonus toggle
* **EvoNet** â€“ arbitrary hidden layers, activation âˆˆÂ {relu,tanh,sigmoid}, optional HebbianÂ Î”W
* **CurriculumEnv** â€“ 12â€¯Ã—â€¯12 grid, DFS solvability check, energy budget, genome autoâ€‘mutation

---

## ğŸ“ˆ Observability & metrics

| Metric | Meaning |
| :-- | :-- |
| `aiga_avg_fitness` | Mean generation fitness |
| `aiga_best_fitness` | Elite fitness |
| `aiga_generations_total` | Counter |
| `aiga_curriculum_stage` | 0â€“3 |

Enable profile `telemetry` to autopush â†’ Prometheus â†’ Grafana.
`docker compose --profile telemetry up`.

---

## ğŸ§ª Tests & CI

* **Coverage â‰¥â€¯90â€¯%** in <â€¯0.5â€¯s (`pytestÂ -q`)
* GitHubÂ Actions â†’ lint â†’ test â†’ build â†’ Cosign sign
* **SBOM** via *Syft* (SPDXÂ v3) per release

---

## â˜ï¸ Kubernetes deploy

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: aiga-demo }
spec:
 replicas: 1
 selector: { matchLabels: { app: aiga-demo } }
 template:
  metadata: { labels: { app: aiga-demo } }
  spec:
   containers:
   - name: orchestrator
    image: ghcr.io/montrealai/alpha-aiga:latest@sha256:<signed>
    ports:
    - { containerPort: 8000 }  # API
    - { containerPort: 7862 }  # UI
    readinessProbe:
     httpGet: { path: /health, port: 8000 }
    envFrom: [{ secretRef: { name: aiga-secrets } }]
```

*Helm chart* â†’ `infra/helm/aiga-demo/`.

---

## ğŸ›¡ SOCâ€‘2Â & supplyâ€‘chain

* Cosignâ€‘signed images (`cosign verify â€¦`)
* Runs **nonâ€‘root UIDÂ 1001**, readâ€‘only code volume
* Secrets via K8s / Docker *secrets* (never baked into layers)
* Dependencies hashed (Poetry lock) & validated at runtime 
* SBOM exported; SLSAÂ levelÂ 2 pipeline

---

## ğŸ§© Tinker guide

| Goal | File | Hint |
| --- | --- | --- |
| Bigger populations | `meta_evolver.py` â†’ `pop_size` | Add `--profile gpu` |
| Faster novelty | `Genome.novelty_weight` | TryÂ 0.2 |
| New curriculum stage | `curriculum_env.py` | Extend `_valid_layout` |
| Swap LLM | `config.env` | Any OpenAI model id |
| Automate experimentation | FastAPI â†’ `/evolve/{n}` | Deterministic SHA checkpoint id |
| Manual reset | FastAPI â†’ `POST /reset` | Fresh population |
| Persist progress | FastAPI â†’ `POST /checkpoint` | Atomic save |

---

## ğŸ†˜ FAQ

| Symptom | Fix |
| :-- | :-- |
| â€œDocker not installedâ€ | <https://docs.docker.com/get-docker> |
| Port collision 7862 | Edit host port in compose |
| ARM Mac slow build | Enable **Rosetta** or `./run_aiga_demo.sh --pull` |
| GPU unseen | `sudo apt install nvidia-container-toolkit` & restart Docker |
| Colab URL missing | Reâ€‘run launch cell (ngrok quirk) |

---

## âš–ï¸ License & credits

*Source & assets* Â©Â 2025Â Montreal.AI, released under the **Apacheâ€‘2.0 License**.
Huge thanks to:

* **JeffÂ Clune** â€“ visionary behind AIâ€‘GAs 
* **OpenAI, Anthropic, Google** â€“ openâ€‘sourcing crucial agent tooling 
* OSS maintainers â€“ you make this possible â™¥

> **Alphaâ€‘Factory** â€” forging intelligence that *invents* intelligence.
