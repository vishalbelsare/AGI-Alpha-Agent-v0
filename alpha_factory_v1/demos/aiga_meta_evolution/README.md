<!--
  AIâ€‘GA Metaâ€‘Evolution Demo
  Alphaâ€‘Factoryâ€¯v1Â ğŸ‘ï¸âœ¨ â€” Multiâ€‘Agent **AGENTICâ€¯Î±â€‘AGI**
  Outâ€‘learn Â· Outâ€‘think Â· Outâ€‘strategise Â· Outâ€‘evolve
  Â© 2025 MONTREAL.AI   MIT License
  -------------------------------------------------------------------------------
  This README is intentionally exhaustive: quick-start, deep-dive, SOC-2 rails,
  CI/CD, K8s, observability, troubleshooting, contributor guide, SBOM notice.
-->

# ğŸŒŒ Algorithms That Invent Algorithms â€” **AI-GA Meta-Evolution Demo**

> *â€œWhy hand-craft intelligence when evolution can author it for you?â€*  
> â€” Jeff Clune, **AI-GAs: AI-Generating Algorithms** (2019)  

This is a one-command, browser-based showcase of Cluneâ€™s **Three Pillars**:

| Pillar | Demo realisation |
|--------|------------------|
| **Meta-learning architectures** | Genome encodes *variable hidden-layer list* & activation; mutates via neuro-evolution |
| **Meta-learning the learning algorithms** | Flag toggles **SGD** â†” **fast Hebbian plasticity** inside *EvoNet* |
| **Generating learning environments** | `CurriculumEnv` self-mutates through 4 stages (Line â†’ Zig-zag â†’ Gap â†’ Maze) |

Within 60 s youâ€™ll watch neural networks **rewrite their own blueprint**
*while the world itself mutates to keep them sharp*.

---

*Table of contents â€¢ [(â†‘ back to top)](#)*

- [ğŸš€ Quick start (local Docker)](#-quick-start-local-docker)
- [ğŸ”‘ OpenAI vs offline Mixtral](#-openai-vs-offline-mixtral)
- [ğŸ›  Architecture deep-dive](#-architecture-deep-dive)
- [ğŸ“ˆ Observability & metrics](#-observability--metrics)
- [ğŸ§ª Tests & CI](#-tests--ci)
- [â˜ï¸ Deploying to Kubernetes](#-deploying-to-kubernetes)
- [ğŸ›¡ SOC-2 / supply-chain rails](#-soc2--supply-chain-rails)
- [ğŸ§© Tinker guide](#-tinker-guide)
- [ğŸ†˜ Troubleshooting FAQ](#-troubleshooting-faq)
- [ğŸ¤ Contributing](#-contributing)
- [âš–ï¸ License & credits](#-license--credits)

---

## ğŸš€ Quick start (local Docker)

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/aiga_meta_evolution

# optional flags:  --pull  use signed image from GHCR
#                  --gpu   enable NVIDIA runtime (CUDA â‰¥ 12)
./run_aiga_demo.sh
```

| Component | URL / Port | What you get |
|-----------|-----------|--------------|
| Gradio dashboard | <http://localhost:7862> | Click **Evolve 5 Generations** to iterate |
| FastAPI | <http://localhost:8000/docs> | OpenAPI JSON API |
| Prometheus scrape | <http://localhost:8000/metrics> | `aiga_*` metrics (avg fitness, stage, gen count) |

> **Cold build** â‰¤ 40 s on modern laptop (â‰ˆ 900 MB image).  
> Re-runs are instant (cached layers).

### Minimal prerequisites

* Docker 24 + compose plug-in  
* â‰ˆ 4 GB RAM (8 GB if you bump `pop_size`)  
* **No GPU required** â€“ runs CPU-only by default.

---

## ğŸ”‘ OpenAI vs offline Mixtral

The stack auto-detects `OPENAI_API_KEY` in `config.env`.

| Scenario | Behaviour |
|----------|-----------|
| `OPENAI_API_KEY=` **set** | LLM commentary & planning via OpenAI Agents SDK |
| **blank / unset** | Falls back to **Mixtral-8x7B-Instruct** served by Ollama side-car â€“ runs 100 % offline |

Neither path changes core evolution logic; LLMs are *assistants*, not oracles.

---

## ğŸ›  Architecture deep-dive

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Docker compose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ orchestrator  (FastAPI + Gradio)   â”‚
â”‚ ollama       (Mixtral fallback)    â”‚
â”‚ prometheus   (optional profile)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–² REST / WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MetaEvolver  â† population JSON ckptâ”‚
â”‚  â”œâ”€ Ray / mp evaluation pool       â”‚
â”‚  â””â”€ EvoNet (torch)   â”€â”€â”           â”‚
â”‚                        â–¼ obs       â”‚
â”‚ CurriculumEnv (Gymnasium)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **MetaEvolver**  
  * Population 24, tournament-k 3, elitism 2, novelty search option  
  * Checkpoint every generation (`/data/checkpoints/evolver_gen####.json`)  
  * SHA-256 digest of population for audit  

* **EvoNet**  
  * Variable hidden layers (tuple), activation from `{relu, tanh, sigmoid}`  
  * Optional per-step Hebbian update (`Î”w = Î· Â· h xáµ€`)  

* **CurriculumEnv**  
  * Grid-world size 12Ã—12, solvability checked via DFS, energy budget  
  * Mutates genome when mastered (< 50 % steps for 5 consecutive episodes)  

---

## ğŸ“ˆ Observability & metrics

Metric | Type | Description
-------|------|------------
`aiga_avg_fitness` | gauge | Mean fitness of last generation
`aiga_best_fitness` | gauge | Elite fitness (stage-independent)
`aiga_generations_total` | counter | Total generations evolved
`aiga_curriculum_stage` | gauge | 0â€“3 (Line â†’ Maze)
`process_*` | gauge | Standard Prometheus process metrics

Enable full stack (`--profile telemetry`) to auto-scrape with Prometheus +
OpenTelemetry Collector. Graph fitness in Grafana or hit the `/metrics` endpoint
directly.

---

## ğŸ§ª Tests & CI

* **Branch coverage â‰¥ 90 %** < 0.5 s (`pytest -q`)  
* GitHub Actions (`.github/workflows/ci.yml`) runs: lint â†’ tests â†’ Docker build  
* **SBOM** generated via Syft, uploaded as job artifact.

Run locally:

```bash
pip install -r ../../requirements-dev.txt
pytest -q
coverage html  # optional report
```

---

## â˜ï¸ Deploying to Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: aiga-demo }
spec:
  replicas: 1
  selector: { matchLabels: { app: aiga-demo } }
  template:
    metadata: { labels: { app: aiga-demo } }
    spec:
      containers:
      - name: orchestrator
        image: ghcr.io/montrealai/alpha-aiga:latest@sha256:<signed>
        ports:
        - { containerPort: 8000 }  # API
        - { containerPort: 7862 }  # UI
        readinessProbe:
          httpGet: { path: /health, port: 8000 }
        envFrom: [{ secretRef: { name: aiga-secrets } }]
```

*Helm chart* lives under `infra/helm/aiga-demo/`.

Horizontal scaling:

```bash
kubectl scale deploy aiga-demo --replicas=4     # Ray will auto-cluster
```

---

## ğŸ›¡ SOC-2 / supply-chain rails

* **Cosign-signed image** (`cosign verify â€¦`) â€“ enforced by `docker-compose.aiga.yml`  
* Non-root UID `1001`, read-only code volume, `/data` for checkpoints only  
* Secrets via Docker/K8s *secrets* (NOT env baked into layers)  
* SBOM (SPDX v3) published per release tag  
* Dependency list locked with Poetry & hash-checked at runtime  

---

## ğŸ§© Tinker guide

| Goal | Touch-point | Hint |
|------|-------------|------|
| Bigger populations | `MetaEvolver(pop_size=â€¦)` | Increase Ray workers or `--profile gpu` |
| Faster convergence | Tune mutation rates (`Genome.mutate`) | Try `novelty_weight â‰ˆ 0.2` |
| New curriculum stage | Append in `CurriculumEnv._valid_layout` | Guarantee solvability via `_is_reachable` |
| Swap LLM | edit `config.env` â†’ `MODEL_NAME=` | Any OpenAI Agents-SDK model ID |
| Plug into trading bot | Use JSON API `/evolve/{n}` + `/checkpoint/latest` | Deterministic SHA id for compliance |

---

## ğŸ†˜ Troubleshooting FAQ

| Symptom | Remedy |
|---------|--------|
| â€œDocker not installedâ€ | <https://docs.docker.com/get-docker> |
| Port 7862 already in use | Change host port in `docker-compose.aiga.yml` |
| Build slow on ARM Mac | Enable **Rosetta** or use `./run_aiga_demo.sh --pull` |
| GPU not detected | `sudo apt install nvidia-container-toolkit` â†’ restart Docker |
| Colab public URL missing | Re-run launch cell; ngrok occasionally throttles |

---

## âš–ï¸ License & credits

*Code & assets* MIT-licensed. Refer to `LICENSE` for full text.  
Heavy thanks to:

* **Jeff Clune** â€“ for the audacious AI-GA roadmap  
* **OpenAI / Anthropic / Google** â€“ open-sourcing pivotal agent tooling  
* Every OSS maintainer whose work this demo stands on

> **Alpha-Factory** â€” forging intelligence that *invents* intelligence.
