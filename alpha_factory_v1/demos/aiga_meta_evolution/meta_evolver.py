# SPDX-License-Identifier: Apache-2.0
# alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py
# © 2025 MONTREAL.AI  Apache-2.0 License
"""
This module is part of a conceptual research prototype. References to
'AGI' or 'superintelligence' describe aspirational goals and do not
indicate the presence of real general intelligence. Use at your own risk.

MetaEvolver v3.0 (2025‑04‑23)
=================================
✦ **Mission**  Self‑contained, SOC‑2‑aligned neuro‑evolution engine that scales
  from a laptop to a Kubernetes Ray cluster with zero code changes.
✦ **Pillar coverage**  Architecture, plasticity rule, and environment co‑evolve.
✦ **Observability**  Prometheus gauges, structured logs, population SHA.
✦ **Fail‑safe parallelism**  Ray ➜ multiprocessing ➜ ThreadPool cascade.
✦ **Audit hooks**  JSON checkpoints (atomic), population hash, deterministic RNG.
✦ **Extensibility**  Plug‑in novelty metrics, LLM commentary, A2A broadcast.
"""
from __future__ import annotations

import copy
import dataclasses as dc
import hashlib
import json
import logging
import math
import os
import pathlib
import random
import contextlib
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from datetime import datetime, timezone
from functools import cached_property
from typing import Callable, List, Tuple

try:
    import numpy as np
except ModuleNotFoundError:  # pragma: no cover - lightweight fallback
    np = None

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    _TORCH = True
except ModuleNotFoundError:  # pragma: no cover - CPU-only stub
    import types

    torch = types.SimpleNamespace(
        Tensor=float,
        device=lambda *_: "cpu",
        cuda=lambda: False,
        zeros_like=lambda *_: 0,
        bmm=lambda *_: 0,
        multinomial=lambda *_: 0,
        no_grad=contextlib.nullcontext,
    )

    class _DummyNN:
        Module = object

        def Linear(*_, **__):
            return None

        def Sequential(*_, **__):
            return []

    nn = _DummyNN()

    class _F:
        relu = staticmethod(lambda x: x)
        gelu = staticmethod(lambda x: x)

    F = _F
    _TORCH = False

# optional deps ------------------------------------------------------------
try:
    import ray

    _HAS_RAY = True
except ImportError:
    _HAS_RAY = False
try:
    from prometheus_client import Gauge

    _fitness_gauge = Gauge("aiga_avg_fitness", "Average fitness per generation")
except ImportError:
    _fitness_gauge = None
try:
    from a2a import A2ASocket

    _A2A = A2ASocket(
        host=os.getenv("A2A_HOST", "localhost"),
        port=int(os.getenv("A2A_PORT", "5555")),
        app_id="meta_evolver",
    )
except Exception:
    _A2A = None

# logging ------------------------------------------------------------------
LOG = logging.getLogger("MetaEvolver")
if not LOG.hasHandlers():
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("%(asctime)sZ %(levelname)s %(message)s", "%Y-%m-%dT%H:%M:%S"))
    LOG.addHandler(h)
    LOG.setLevel(os.getenv("LOGLEVEL", "INFO").upper())

# global config ------------------------------------------------------------
if _TORCH:
    Device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    _ACT: dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
        "relu": F.relu,
        "tanh": torch.tanh,
        "sigmoid": torch.sigmoid,
        "gelu": F.gelu,
    }
else:  # pragma: no cover - minimal placeholders
    Device = "cpu"
    _ACT = {"relu": lambda x: x, "tanh": lambda x: x, "sigmoid": lambda x: x, "gelu": lambda x: x}
CHKPT_DIR = pathlib.Path(os.getenv("CHECKPOINT_DIR", "./checkpoints"))
CHKPT_DIR.mkdir(parents=True, exist_ok=True)


# ────────────────────────────── Genome ────────────────────────────────────
@dc.dataclass(slots=True)
class Genome:
    layers: Tuple[int, ...] = (32,)
    activation: str = "relu"
    hebbian: bool = False
    novelty_weight: float = 0.0  # 0–1

    # evo‑ops -------------------------------------------------------------
    def mutate(self) -> "Genome":
        g = copy.deepcopy(self)
        if random.random() < 0.4:
            idx = random.randrange(len(g.layers))
            delta = random.randint(-8, 8)
            new_size = max(4, min(128, g.layers[idx] + delta))
            layers = list(g.layers)
            layers[idx] = new_size
            if random.random() < 0.2 and len(layers) < 4:
                layers.insert(idx, random.choice([16, 32, 64]))
            g.layers = tuple(layers)
        if random.random() < 0.2:
            g.activation = random.choice(list(_ACT))
        if random.random() < 0.1:
            g.hebbian = not g.hebbian
        if random.random() < 0.15:
            g.novelty_weight = round(min(1.0, max(0.0, g.novelty_weight + random.uniform(-0.15, 0.15))), 2)
        return g

    # serialisation -------------------------------------------------------
    def to_json(self) -> str:
        return json.dumps(dc.asdict(self), separators=(",", ":"))

    @staticmethod
    def from_json(js: str | dict) -> "Genome":
        return Genome(**(json.loads(js) if isinstance(js, str) else js))

    @cached_property
    def sha(self) -> str:
        return hashlib.sha256(self.to_json().encode()).hexdigest()[:12]


# ───────────────────────── network wrapper ────────────────────────────────
class EvoNet(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int, g: Genome):
        super().__init__()
        last, modules = obs_dim, []
        for h in g.layers:
            modules.append(nn.Linear(last, h))
            last = h
        modules.append(nn.Linear(last, act_dim))
        self.model = nn.ModuleList(modules)
        self.genome = g
        if g.hebbian:
            self.hFast = torch.zeros_like(next(self.model.parameters()))
        self._init()

    def _init(self):
        for m in self.model:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor):
        act_fn = _ACT[self.genome.activation]
        h = x
        for idx, layer in enumerate(self.model):
            h = act_fn(layer(h))
            if self.genome.hebbian and idx < len(self.model) - 1:
                with torch.no_grad():
                    dw = 0.03 * torch.bmm(h.unsqueeze(2), x.unsqueeze(1))
                    self.hFast = (self.hFast + dw.mean(0)).clamp(-0.02, 0.02)
                    layer.weight.data += self.hFast
        return h


# ────────────────────────── MetaEvolver core ──────────────────────────────
class MetaEvolver:
    def __init__(
        self,
        env_cls: Callable,
        pop_size: int = 32,
        elitism: int = 2,
        parallel: bool = True,
        checkpoint_dir: pathlib.Path = CHKPT_DIR,
        llm: Callable[[str], str] | None = None,
        *,
        start_socket: bool = False,
    ) -> None:
        """Create a meta-evolution engine.

        Args:
            env_cls: Environment class to instantiate per evaluation.
            pop_size: Number of genomes per generation.
            elitism: How many top genomes carry over unchanged.
            parallel: Whether to evaluate genomes in parallel.
            checkpoint_dir: Directory for checkpoints.
            llm: Optional LLM callback for commentary.
            start_socket: Start the optional A2A socket.
        """
        self.env_cls, self.pop_size, self.elitism = env_cls, pop_size, elitism
        self.parallel = parallel
        self.ckpt_dir = pathlib.Path(checkpoint_dir)
        self.ckpt_dir.mkdir(parents=True, exist_ok=True)
        self.llm = llm
        self.rng = random.Random(int("A1GA", 16))
        self.gen = 0
        self.history: List[Tuple[int, float]] = []
        self._archive: List[np.ndarray] = []
        self._best_fitness = -math.inf
        self.best_genome: Genome | None = None
        self._last_scores: List[float] = []
        self._init_population()
        if self.parallel and _HAS_RAY and not ray.is_initialized():
            ray.init(ignore_reinit_error=True, _temp_dir=str(self.ckpt_dir / "ray"))
        if start_socket:
            self.start_socket()
        LOG.info("Evolver ready ▶ pop=%d device=%s", self.pop_size, Device)

    # population -----------------------------------------------------------
    def _init_population(self):
        seed = Genome()
        self.population = [seed.mutate() for _ in range(self.pop_size)]
        self.best_genome = self.population[0]

    # A2A -----------------------------------------------------------------
    @staticmethod
    def start_socket() -> None:
        """Start the optional A2A socket if available."""
        if _A2A:
            try:
                _A2A.start()
            except Exception:  # pragma: no cover - best effort
                LOG.warning("Failed to start A2A socket", exc_info=True)

    # evaluation util ------------------------------------------------------
    def _simulate(self, g: Genome) -> Tuple[float, np.ndarray]:
        env = self.env_cls()
        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.n
        net = EvoNet(obs_dim, act_dim, g).to(Device)
        obs, _ = env.reset()
        total, bc = 0.0, []
        for _ in range(env.genome.max_steps):
            with torch.no_grad():
                a = net(torch.tensor(obs, dtype=torch.float32, device=Device)).argmax().item()
            obs, rew, done, truncated, _ = env.step(a)
            total += rew
            bc.append(obs)
            if done or truncated:
                break
        bc_vec = np.mean(bc, axis=0)
        if g.novelty_weight and self._archive:
            novelty = float(np.mean([np.linalg.norm(bc_vec - a) for a in self._archive]))
            total += g.novelty_weight * novelty
        return total, bc_vec

    @staticmethod
    def _simulate_worker(env_cls, archive, js: str):
        g = Genome.from_json(js)
        env = env_cls()
        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.n
        net = EvoNet(obs_dim, act_dim, g).to(Device)
        obs, _ = env.reset()
        total, bc = 0.0, []
        for _ in range(env.genome.max_steps):
            with torch.no_grad():
                a = net(torch.tensor(obs, dtype=torch.float32, device=Device)).argmax().item()
            obs, rew, done, truncated, _ = env.step(a)
            total += rew
            bc.append(obs)
            if done or truncated:
                break
        bc_vec = np.mean(bc, axis=0)
        if g.novelty_weight and archive:
            novelty = float(np.mean([np.linalg.norm(bc_vec - a) for a in archive]))
            total += g.novelty_weight * novelty
        return total, bc_vec

    # -------- parallel dispatch ------------------------------------------
    def _evaluate_population(self) -> List[float]:
        if self.parallel and _HAS_RAY:
            return self._ray_eval()
        return self._mp_eval() if self.parallel else self._thread_eval()

    def _ray_eval(self):
        env_cls = self.env_cls
        archive = self._archive.copy()

        @ray.remote
        def _worker(js: str):
            return MetaEvolver._simulate_worker(env_cls, archive, js)

        futures = [_worker.remote(g.to_json()) for g in self.population]
        results = ray.get(futures)
        return self._post_eval(results)

    def _mp_eval(self):
        with ProcessPoolExecutor() as pool:
            results = list(pool.map(self._simulate, self.population))
        return self._post_eval(results)

    def _thread_eval(self):
        with ThreadPoolExecutor() as pool:
            results = list(pool.map(self._simulate, self.population))
        return self._post_eval(results)

    def _post_eval(self, results):
        scores, bcs = zip(*results)
        self._archive.extend(bcs[-64:])
        return list(scores)

    # tournament -----------------------------------------------------------
    def _select(self, scores, k=3):
        idx = max(random.sample(range(self.pop_size), k), key=lambda i: scores[i])
        return self.population[idx]

    # evolutionary loop ----------------------------------------------------
    def run_generations(self, n: int = 5):
        """Evolve the population for ``n`` generations."""
        for _ in range(n):
            scores = self._evaluate_population()
            self._last_scores = scores
            best_idx = int(np.argmax(scores))
            if scores[best_idx] > self._best_fitness:
                self._best_fitness = scores[best_idx]
                self.best_genome = self.population[best_idx]
            avg = float(np.mean(scores))
            self.history.append((self.gen, avg))
            if _fitness_gauge:
                _fitness_gauge.set(avg)
            LOG.info("gen=%d avg=%.3f best=%.2f", self.gen, avg, self._best_fitness)
            if _A2A:
                _A2A.sendjson({"gen": self.gen, "avg": avg, "sha": self.population_sha()})
            elite_idx = sorted(range(self.pop_size), key=lambda i: scores[i], reverse=True)[: self.elitism]
            new_pop = [self.population[i] for i in elite_idx]
            while len(new_pop) < self.pop_size:
                new_pop.append(self._select(scores).mutate())
            self.population = new_pop
            self.gen += 1
            self._save()

    # checkpoint -----------------------------------------------------------
    def _save(self):
        data = {
            "gen": self.gen,
            "pop": [g.to_json() for g in self.population],
            "hist": self.history,
            "arc": [a.tolist() for a in self._archive[-256:]],
            "seed": self.rng.random(),
            "sha": self.population_sha(),
            "best_fitness": self._best_fitness,
            "best_genome": self.best_genome.to_json() if self.best_genome else None,
            "ts": datetime.now(timezone.utc).isoformat(),
        }
        p = self.ckpt_dir / f"gen_{self.gen:04d}.json.tmp"
        p.write_text(json.dumps(data))
        p.replace(p.with_suffix(""))

    def save(self) -> None:
        """Public wrapper for checkpoint persistence."""
        self._save()

    def reset(self) -> None:
        """Return to generation zero with a fresh population and clear checkpoints."""
        # Clear in-memory state
        self._init_population()
        self.gen = 0
        self.history.clear()
        self._archive.clear()
        self._best_fitness = -math.inf
        self.best_genome = self.population[0]
        self._last_scores.clear()

    def load(self, path: pathlib.Path | None = None):
        """Restore state from ``path`` or the latest checkpoint."""
        if path is None:
            latest = max(self.ckpt_dir.glob("gen_*.json"), default=None)
            if not latest:
                raise FileNotFoundError("no checkpoint found")
            path = latest
        js = json.loads(path.read_text())
        self.gen = js["gen"]
        self.population = [Genome.from_json(j) for j in js["pop"]]
        self.history = js.get("hist", [])
        self._archive = [np.array(a) for a in js.get("arc", [])]
        self.rng.seed(js.get("seed", 0))
        self._best_fitness = js.get("best_fitness", -math.inf)
        bg = js.get("best_genome")
        self.best_genome = Genome.from_json(bg) if bg else self.population[0]
        if _fitness_gauge:
            _fitness_gauge.set(self._best_fitness)
        LOG.info("Loaded checkpoint gen=%d sha=%s", self.gen, js.get("sha", "?"))

    # utils ----------------------------------------------------------------
    def population_sha(self) -> str:
        """Return a hash of the current population."""
        concat = "".join(sorted(g.sha for g in self.population))
        return hashlib.sha256(concat.encode()).hexdigest()[:16]

    def history_plot(self):
        """Return the fitness history as a ``pandas`` DataFrame."""
        import pandas as pd

        return pd.DataFrame(self.history, columns=["generation", "avg_fitness"])

    def latest_log(self):
        """Return a textual summary of the best genome."""
        champ = self.best_genome or max(self.population, key=lambda g: sum(g.layers))
        msg = f"Champion {champ.sha}: {champ.to_json()}"
        if self.llm:
            msg += "\n" + self.llm(f"Critique genome {champ.to_json()} in ≤30 words.")
        return msg

    @property
    def best_fitness(self) -> float:
        """Best fitness score observed so far."""
        return self._best_fitness

    @property
    def best_architecture(self) -> str:
        """JSON representation of the champion genome."""
        return self.best_genome.to_json() if self.best_genome else ""


def cli() -> None:
    """Run a short evolutionary loop and print the champion genome."""
    import argparse

    parser = argparse.ArgumentParser(description="AI-GA Meta-Evolver demo")
    parser.add_argument("--gens", type=int, default=5, help="Generations to run")
    args = parser.parse_args()

    if np is None or not _TORCH:
        print("Champion: stub")
        return

    from curriculum_env import CurriculumEnv

    evolver = MetaEvolver(env_cls=CurriculumEnv, start_socket=True)
    evolver.run_generations(args.gens)
    print(evolver.latest_log())

    try:
        df = evolver.history_plot()
        print(df.tail())
    except Exception:  # pragma: no cover - pandas optional
        pass


if __name__ == "__main__":  # pragma: no cover
    cli()
