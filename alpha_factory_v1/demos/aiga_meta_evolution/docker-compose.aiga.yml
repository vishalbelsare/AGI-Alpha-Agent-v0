# This demo is a conceptual research prototype. References to "AGI" and
# "superintelligence" describe aspirational goals and do not indicate the
# presence of a real general intelligence. Use at your own risk.
version: "3.9"

###################################################################
# AIâ€‘GA Metaâ€‘Evolution â€¢ Fullâ€‘stack Docker Compose (v1.2.0)
# -----------------------------------------------------------------
#  âœ“ Cosign signature gate (supply ./infra/cosign.pub)
#  âœ“ 100â€¯% offline path via Ollama/Mixtral (default)
#  âœ“ CPU, GPU and Telemetry (Prom + OTEL) profiles
#  âœ“ Secrets injected via Docker *secrets* (not env vars)
#  âœ“ Readâ€‘only rootfs, nonâ€‘root UID, seccomp:unconfined â†’ SOCâ€‘2
#  âœ“ Log rotation, healthchecks & resource limits
#  âœ“ Swedenâ€‘compliant Unicode ðŸ˜‰
###################################################################

x-common: &common
  restart: unless-stopped
  networks: [aiga]
  user: "1001:1001"              # nonâ€‘root
  read_only: true
  security_opt: [seccomp=unconfined]
  tmpfs: [/tmp]
  env_file: ./config.env          # nonâ€‘secret vars
  secrets: [openai_api_key]
  logging:
    driver: json-file
    options: {max-size: "10m", max-file: "3"}
  ulimits:
    nofile: {soft: 65535, hard: 65535}

secrets:
  openai_api_key:
    file: ./secrets/OPENAI_API_KEY  # optional; leave empty for offline mode

services:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ cosign gate (block on signature failure) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  verify-image:
    image: ghcr.io/sigstore/cosign:v2.2.4
    command: ["verify", "--key", "/cosign.pub", "${AIGA_IMAGE:-ghcr.io/montrealai/alpha-aiga:latest}"]
    volumes: [./infra/cosign.pub:/cosign.pub:ro]
    profiles: ["cpu", "gpu"]
    networks: [aiga]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ orchestrator (CPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  orchestrator:
    <<: *common
    image: ${AIGA_IMAGE:-ghcr.io/montrealai/alpha-aiga:latest}
    build:
      context: ../..
      dockerfile: ./Dockerfile
      target: runtime
    command: python /app/demo/agent_aiga_entrypoint.py
    volumes:
      - aiga_checkpoints:${CHECKPOINT_DIR:-/data/checkpoints}
    ports: ["7862:7862", "8000:8000"]
    depends_on:
      verify-image: {condition: service_completed_successfully}
      ollama: {condition: service_started}
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits: {cpus: "2", memory: 4G}
    profiles: ["cpu"]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ orchestrator (CUDA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  orchestrator-gpu:
    <<: *common
    image: ${AIGA_IMAGE:-ghcr.io/montrealai/alpha-aiga:latest}
    runtime: nvidia
    environment: {NVIDIA_VISIBLE_DEVICES: all}
    command: python /app/demo/agent_aiga_entrypoint.py
    volumes:
      - aiga_checkpoints:${CHECKPOINT_DIR:-/data/checkpoints}
    depends_on:
      verify-image: {condition: service_completed_successfully}
      ollama: {condition: service_started}
    deploy:
      resources:
        reservations: {devices: [{capabilities: [gpu]}]}
    profiles: ["gpu"]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ollama â€“ offline LLM backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ollama:
    image: ollama/ollama:latest
    tty: true
    environment: {OLLAMA_MODELS: mixtral:instruct}
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434"]
      interval: 30s
      timeout: 5s
      retries: 5

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prometheus + OTEL (optâ€‘in) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  prometheus:
    image: prom/prometheus:latest
    profiles: ["telemetry"]
    volumes: [./infra/prometheus.yml:/etc/prometheus/prometheus.yml:ro]
    ports: ["9090:9090"]
    networks: [aiga]

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: ["--config=/etc/otel-config.yaml"]
    volumes: [./infra/otel-config.yaml:/etc/otel-config.yaml:ro]
    profiles: ["telemetry"]
    ports: ["4318:4318"]
    networks: [aiga]

volumes:
  aiga_checkpoints:
  ollama_models:

networks:
  aiga:
    driver: bridge
