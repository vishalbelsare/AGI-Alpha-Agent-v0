# SPDX-License-Identifier: Apache-2.0
"""agent_aiga_entrypoint.py – AI‑GA Meta‑Evolution Service
================================================================
Production‑grade entry point that wraps the *MetaEvolver* demo into a
Kubernetes‑/Docker‑friendly micro‑service with:
• **FastAPI** HTTP API (health, metrics, evolve, checkpoint, best‑alpha)
• **Gradio** dashboard on *:7862* for non‑technical users
• **Prometheus** metrics + optional **OpenTelemetry** traces
• Optional **ADK** registration + **A2A** mesh socket (auto‑noop if libs absent)
• Fully offline when `OPENAI_API_KEY` is missing – falls back to Ollama/Mistral
• Atomic checkpointing & antifragile resume (SIGTERM‑safe)
• SBOM‑ready logging + SOC‑2 log hygiene

The file is *self‑contained*; **no existing behaviour removed** – only
additive hardening to satisfy enterprise infosec & regulator audits.
"""
from __future__ import annotations

import os, asyncio, signal, logging, time, json
from pathlib import Path
from typing import Any, Dict

import uvicorn
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from prometheus_client import Counter, Gauge, Histogram, generate_latest, CONTENT_TYPE_LATEST

# optional‑imports block keeps runtime lean
try:
    from opentelemetry import trace  # type: ignore
    from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor  # type: ignore
except ImportError:  # pragma: no cover
    trace = None  # type: ignore
    FastAPIInstrumentor = None  # type: ignore

try:
    from adk.runtime import AgentRuntime  # type: ignore
except ImportError:  # pragma: no cover
    AgentRuntime = None  # type: ignore

try:
    from a2a import A2ASocket  # type: ignore
except ImportError:  # pragma: no cover
    A2ASocket = None  # type: ignore

try:  # optional dependency
    from openai_agents import OpenAIAgent, Tool
except ImportError as exc:  # pragma: no cover - missing package
    raise SystemExit("openai_agents package is required. Install with `pip install openai-agents`") from exc
try:
    from alpha_factory_v1.backend import adk_bridge
except Exception:  # pragma: no cover - optional dependency
    adk_bridge = None
if __package__ is None:
    import sys
    from pathlib import Path

    sys.path.append(str(Path(__file__).resolve().parent))
    __package__ = "alpha_factory_v1.demos.aiga_meta_evolution"

from .openai_agents_bridge import EvolverAgent
from .meta_evolver import MetaEvolver
from .curriculum_env import CurriculumEnv
import gradio as gr

try:  # optional JWT auth
    import jwt  # type: ignore
except Exception:  # pragma: no cover - optional
    jwt = None  # type: ignore

# ---------------------------------------------------------------------------
# CONFIG --------------------------------------------------------------------
# ---------------------------------------------------------------------------
SERVICE_NAME = os.getenv("SERVICE_NAME", "aiga-meta-evolution")
GRADIO_PORT = int(os.getenv("GRADIO_PORT", "7862"))
API_PORT = int(os.getenv("API_PORT", "8000"))
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OLLAMA_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434/v1")
MAX_GEN = int(os.getenv("MAX_GEN", "1000"))  # safety rail
ENABLE_OTEL = os.getenv("ENABLE_OTEL", "false").lower() == "true"
ENABLE_SENTRY = os.getenv("ENABLE_SENTRY", "false").lower() == "true"
SENTRY_DSN = os.getenv("SENTRY_DSN", "")
RATE_LIMIT = int(os.getenv("RATE_LIMIT_PER_MIN", "120"))
AUTH_TOKEN = os.getenv("AUTH_BEARER_TOKEN")
JWT_PUBLIC_KEY = os.getenv("JWT_PUBLIC_KEY")
JWT_ISSUER = os.getenv("JWT_ISSUER", "aiga.local")

SAVE_DIR = Path(os.getenv("CHECKPOINT_DIR", "/data/checkpoints"))
SAVE_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------------------------------------------------------------
# LOGGING --------------------------------------------------------------------
# ---------------------------------------------------------------------------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s %(levelname)s %(name)s | %(message)s",
)
log = logging.getLogger(SERVICE_NAME)

if ENABLE_SENTRY and SENTRY_DSN:
    try:
        import sentry_sdk  # type: ignore

        sentry_sdk.init(dsn=SENTRY_DSN, traces_sample_rate=1.0)
        log.info("Sentry enabled")
    except ImportError:  # pragma: no cover - optional
        log.warning("Sentry requested but sentry_sdk missing")

# ---------------------------------------------------------------------------
# METRICS --------------------------------------------------------------------
# ---------------------------------------------------------------------------
FITNESS_GAUGE = Gauge("aiga_best_fitness", "Best fitness achieved so far")
GEN_COUNTER = Counter("aiga_generations_total", "Total generations processed")
STEP_LATENCY = Histogram("aiga_step_seconds", "Seconds spent per evolution step")
REQUEST_COUNTER = Counter("aiga_http_requests", "API requests", ["route"])

# rate-limit state
_REQUEST_LOG: dict[str, list[float]] = {}

# ---------------------------------------------------------------------------
# LLM TOOLING ----------------------------------------------------------------
# ---------------------------------------------------------------------------
LLM = OpenAIAgent(
    model=MODEL_NAME,
    api_key=OPENAI_API_KEY,
    base_url=(None if OPENAI_API_KEY else OLLAMA_URL),
)


@Tool(name="describe_candidate", description="Explain why this architecture might learn fast")
async def describe_candidate(arch: str):
    return await LLM(f"In two sentences, explain why architecture '{arch}' might learn quickly.")


# ---------------------------------------------------------------------------
# CORE RUNTIME ---------------------------------------------------------------
# ---------------------------------------------------------------------------
class AIGAMetaService:
    """Thread‑safe façade around *MetaEvolver*."""

    def __init__(self) -> None:
        self._lock = asyncio.Lock()
        self.evolver = MetaEvolver(
            env_cls=CurriculumEnv,
            llm=LLM,
            checkpoint_dir=SAVE_DIR,
            start_socket=True,
        )
        self._restore_if_any()

    # -------- public ops --------
    async def evolve(self, gens: int = 1) -> None:
        async with self._lock:
            start = time.perf_counter()
            self.evolver.run_generations(gens)
            GEN_COUNTER.inc(gens)
            FITNESS_GAUGE.set(self.evolver.best_fitness)
            STEP_LATENCY.observe(time.perf_counter() - start)

    async def checkpoint(self) -> None:
        async with self._lock:
            self.evolver.save()

    async def reset(self) -> None:
        """
        Reset the state of the MetaEvolver to its initial configuration.

        This method is thread-safe and uses a lock to prevent concurrent
        modifications to the evolver's state.
        """
        async with self._lock:
            self.evolver.reset()

    async def best_alpha(self) -> Dict[str, Any]:
        arch = self.evolver.best_architecture
        summary = await describe_candidate(arch)
        return {"architecture": arch, "fitness": self.evolver.best_fitness, "summary": summary}

    # -------- helpers --------
    def _restore_if_any(self) -> None:
        try:
            self.evolver.load()
            log.info("restored state → best fitness %.4f", self.evolver.best_fitness)
        except FileNotFoundError:
            log.info("no prior checkpoint – fresh run")

    # -------- dashboard helpers --------
    def history_plot(self):
        return self.evolver.history_plot()

    def latest_log(self):
        return self.evolver.latest_log()


service = AIGAMetaService()

# ---------------------------------------------------------------------------
# FASTAPI --------------------------------------------------------------------
# ---------------------------------------------------------------------------
app = FastAPI(title="AI‑GA Meta‑Evolution API", version="1.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if ENABLE_OTEL and FastAPIInstrumentor:
    FastAPIInstrumentor.instrument_app(app)

# ---------- routes ----------


@app.middleware("http")
async def _count_requests(request, call_next):
    path = request.url.path
    if path.startswith("/metrics"):
        return await call_next(request)
    # -------- auth gate --------
    if AUTH_TOKEN or JWT_PUBLIC_KEY:
        header = request.headers.get("authorization")
        if not header:
            return JSONResponse({"detail": "unauthorized"}, status_code=401)
        scheme, _, token = header.partition(" ")
        if scheme.lower() != "bearer":
            return JSONResponse({"detail": "unauthorized"}, status_code=401)
        if AUTH_TOKEN and token == AUTH_TOKEN:
            pass
        elif JWT_PUBLIC_KEY and jwt:
            try:
                jwt.decode(token, JWT_PUBLIC_KEY, algorithms=["RS256"], issuer=JWT_ISSUER)
            except Exception:
                return JSONResponse({"detail": "unauthorized"}, status_code=401)
        else:
            return JSONResponse({"detail": "unauthorized"}, status_code=401)
    REQUEST_COUNTER.labels(route=path).inc()
    ip = request.client.host
    now = time.time()
    window = now - 60
    times = [t for t in _REQUEST_LOG.get(ip, []) if t > window]
    if len(times) >= RATE_LIMIT:
        return JSONResponse({"detail": "rate limit exceeded"}, status_code=429)
    times.append(now)
    _REQUEST_LOG[ip] = times
    return await call_next(request)


@app.get("/health")
async def read_health():
    return {
        "status": "ok",
        "generations": int(GEN_COUNTER._value.get()),
        "best_fitness": service.evolver.best_fitness,
    }


@app.get("/metrics")
async def metrics():
    return generate_latest(), 200, {"Content-Type": CONTENT_TYPE_LATEST}


@app.post("/evolve/{gens}")
async def evolve_endpoint(gens: int, background_tasks: BackgroundTasks):
    if gens < 1 or gens > MAX_GEN:
        raise HTTPException(400, f"gens must be 1–{MAX_GEN}")
    background_tasks.add_task(service.evolve, gens)
    return {"msg": f"scheduled evolution for {gens} generations"}


@app.post("/checkpoint")
async def checkpoint_endpoint(background_tasks: BackgroundTasks):
    background_tasks.add_task(service.checkpoint)
    return {"msg": "checkpoint scheduled"}


@app.post("/reset")
async def reset_endpoint(background_tasks: BackgroundTasks):
    background_tasks.add_task(service.reset)
    return {"msg": "reset scheduled"}


@app.get("/alpha")
async def best_alpha():
    """Return current best architecture + LLM summary (meta‑explanation)."""
    return await service.best_alpha()


# ---------------------------------------------------------------------------
# GRADIO DASHBOARD -----------------------------------------------------------
# ---------------------------------------------------------------------------
async def _launch_gradio() -> None:  # noqa: D401
    with gr.Blocks(title="AI‑GA Meta‑Evolution Demo") as ui:
        plot = gr.LinePlot(label="Fitness by Generation")
        log_md = gr.Markdown()

        def on_step(g=5):
            asyncio.run(service.evolve(g))
            return service.history_plot(), service.latest_log()

        gr.Button("Evolve 5 Generations").click(on_step, [], [plot, log_md])
    ui.launch(server_name="0.0.0.0", server_port=GRADIO_PORT, share=False)


# ---------------------------------------------------------------------------
# SIGNAL HANDLERS ------------------------------------------------------------
# ---------------------------------------------------------------------------
async def _graceful_exit(*_):
    log.info("SIGTERM received – persisting state …")
    await service.checkpoint()
    loop = asyncio.get_event_loop()
    loop.stop()


# ---------------------------------------------------------------------------
# MAIN -----------------------------------------------------------------------
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    for sig in (signal.SIGTERM, signal.SIGINT):
        loop.add_signal_handler(sig, lambda s=sig: asyncio.create_task(_graceful_exit(s)))

    # start Gradio dashboard asynchronously
    loop.create_task(_launch_gradio())

    # register with agent mesh (optional)
    if AgentRuntime:
        AgentRuntime.register(SERVICE_NAME, f"http://localhost:{API_PORT}")
    if A2ASocket:
        service.evolver.start_socket()
    if adk_bridge and adk_bridge.adk_enabled():
        evolver_agent = EvolverAgent()
        adk_bridge.auto_register([evolver_agent])
        adk_bridge.maybe_launch()

    # run FastAPI (blocking)
    uvicorn.run(app, host="0.0.0.0", port=API_PORT, log_level="info")
