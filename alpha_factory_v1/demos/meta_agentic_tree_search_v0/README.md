# Metaâ€‘Agentic Tree Search (MATS) Demo â€” v0

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/meta_agentic_tree_search_v0/colab_meta_agentic_tree_search.ipynb)

**Abstract:** We pioneer **Meta-Agentic Tree Search (MATS)**, a novel framework for autonomous multi-agent decision optimization in complex strategic domains. MATS enables intelligent agents to collaboratively navigate and optimize high-dimensional strategic search spaces through **recursive agent-to-agent interactions**. In this **second-order agentic** scheme, each agent in the system iteratively refines the intermediate strategies proposed by other agents, yielding a self-improving decision-making process. This recursive optimization mechanism systematically uncovers latent inefficiencies and unexploited opportunities that static or single-agent approaches often overlook.

> **Status:** Experimental Â· Proofâ€‘ofâ€‘Concept Â· Alpha  
> **Location:** `alpha_factory_v1/demos/meta_agentic_tree_search_v0`  
> **Goal:** Showcase how recursive agentâ€‘toâ€‘agent rewrites â€” navigated with a bestâ€‘first tree policy â€” can rapidly surface highâ€‘value trading policies that exploit AGIâ€‘driven discontinuities (â€œAGIÂ Alphaâ€).

## 1â€‚Why this demo exists
Financial edges sourced from AGI inflection points decay in hours or days. Classical research pipelines are too slow.  
MATS compresses the ideaâ€‘toâ€‘capital cycle by letting agents continuously rewrite each other while a Monteâ€‘Carlo tree search focuses compute on the most promising rewrite trajectories.

## 2â€‚Highâ€‘level picture
```
root population
      â”‚  metaâ€‘rewrite
      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   tree policy   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Node k     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Node k+1     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Each edge = â€œone agent improves anotherâ€; backpropagation = â€œwhich rewrite path maximises riskâ€‘adjusted Î±â€.

## 3â€‚Formal definition
*(verbatim from specification for precision)*  

> **Metaâ€‘Agentic Tree Search (MATS)**  
> Let **E** be a partiallyâ€‘observable, stochastic environment parameterised by state vector *s* and reward function *R*.  
> Let **ğ’œâ‚€** = {aâ‚,â€¦,aâ‚™} be a population of base agents, each with policy Ï€áµ¢(Â·|Î¸áµ¢).  
> **Metaâ€‘agents** are higher-order policies **Î  : (ğ’œâ‚€, ğ’®, ğ’­) â†’ ğ’œâ‚€â€²** that rewrite or reâ€‘parameterise base agents to maximise a metaâ€‘objective **J**.  
> A search node **v** is the tuple (ğ’œâ‚–,â€¯Î£â‚–) where ğ’œâ‚– is the current agent pool after *k* rewrites and Î£â‚– their cumulative performance statistics.  
> The tree policy **T** selects the next node via a bestâ€‘first acquisition criterion (e.g. UCB over expected Î±).  
> Terminal nodes are reached when Î”Î±Â <Â Îµ or depthÂ â‰¥Â d\*.  
> **Output**: argmaxâ‚áµ¥âˆˆğ’±_leafâ‚Â J(v).

## 4â€‚Minimal algorithm (reference implementation)
```python
def MATS(root_agents, env, horizon_days):
    tree = Tree(Node(root_agents))
    while resources_left():
        node = tree.select(best_first)                       # â† UCB / Thompson
        improved = meta_rewrite(node.agents, env)           # â† gradient, evo, codeâ€‘gen
        reward = rollouts(improved, env, horizon_days)      # â† riskâ€‘adj Î±
        child = Node(improved, reward)
        tree.add_child(node, child)
        tree.backprop(child)
    return tree.best_leaf().agents
```

### Design knobs
| Component          | Options (demo default) |
|--------------------|------------------------|
| `best_first`       | UCB1, TS, Îµâ€‘greedy (UCB1) |
| `meta_rewrite`     | PPO fineâ€‘tune, CMAâ€‘ES, GPTâ€‘4 codeâ€‘gen (PPO) |
| Reward             | IRR, CumPnL/âˆšVar, Sharpe (IRR) |
| Environment        | Toy number-line env (default), limitâ€‘orderâ€‘book sim, OpenAI Gym trading env |

### 4.1Â Â·Â OpenAI/ADK rewrite option
When the optional `openai-agents` and `google-adk` packages are installed the
demo can leverage a tiny ``RewriterAgent`` built with the OpenAI Agents SDK
together with the A2A protocol to generate candidate policies.  The agent is
instantiated directly from :func:`openai_rewrite` and executed once per tree
step. Enable this behaviour with:

```bash
python run_demo.py --rewriter openai --episodes 500 --model gpt-4o
```
The script automatically falls back to the offline rewriter when the
dependencies are unavailable so the notebook remains runnable anywhere.

When the optional `openai` package is also present, `openai_rewrite` uses
`OpenAI().chat.completions.create` to refine candidate integer policies.  Supply an
`OPENAI_API_KEY` environment variable to activate this behaviour.  Without a
key or in fully offline environments the routine simply increments the
proposed policy elements so the rest of the demo keeps working.  You can
override the model used by setting ``OPENAI_MODEL`` (defaults to ``gpt-4o``).
Output from the model is processed via the ``_parse_numbers`` helper which
extracts integers from freeâ€‘form text and validates their length so the search
loop remains stable even when the LLM response contains extra commentary. When
the output is malformed or incomplete the helper simply increments the previous
policy as a safe fallback. The rewrite routine executes the LLM call via a small
synchronous helper so it functions both with and without an active event loop.

### 4.2Â Â·Â Anthropic rewrite option
When the optional `anthropic` package is installed and an `ANTHROPIC_API_KEY`
environment variable is configured the demo can use Claude models to refine
candidate policies via the ``anthropic_rewrite`` helper. Enable this behaviour
with:

```bash
python run_demo.py --rewriter anthropic --episodes 500 --model claude-3-opus-20240229
```
As with the OpenAI path the call automatically falls back to the offline
rewriter whenever dependencies or API keys are missing so the notebook remains
fully reproducible.

### 4.3 Â· OpenAI Agents bridge
The `openai_agents_bridge.py` script exposes the search loop via the
**OpenAI Agents SDK** and optionally the **Google ADK** federation layer. Launch
the bridge to control the demo through API calls or the Agents runtime UI:

```bash
mats-bridge --help
```
Run a quick environment check with ``--verify-env`` if desired:
```bash
mats-bridge --verify-env --episodes 3 --target 4 --model gpt-4o
```
The bridge exposes a small :func:`verify_env` helper that performs the same
sanity check programmatically. Call it from Python or rely on the command
above. If the `openai_agents` package or API keys are missing the bridge
automatically falls back to running the search loop locally so the notebook
remains reproducible anywhere. When running offline you can still invoke
`run_search` directly to verify the helper logic:

```bash
mats-bridge --episodes 3 --target 4 --model gpt-4o
python -m alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge --episodes 3 --target 4
```
Enable the optional ADK gateway with ``--enable-adk`` (or set
``ALPHA_FACTORY_ENABLE_ADK=true``) to expose the agent over the A2A protocol.
This prints a short completion summary after executing the demo loop.

### 4.4Â Â·Â Google ADK Integration
Install the ``google-adk`` package to communicate over the A2A protocol:

```bash
pip install google-adk
```

Set ``ALPHA_FACTORY_ENABLE_ADK=true`` or pass ``--enable-adk`` to enable the gateway.
The ADK layer is optional so the demo still runs completely offline.

## 5â€‚Quick start
```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/meta_agentic_tree_search_v0
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.lock         # install pinned dependencies
python run_demo.py --verify-env          # optional sanity check
python run_demo.py --config configs/default.yaml --episodes 500 --target 5 --seed 42 --model gpt-4o
# or equivalently
python -m alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo --episodes 500 --target 5
# installed script
mats-bridge --episodes 3
```
`run_demo.py` prints a perâ€‘episode scoreboard.  Pass `--log-dir logs` to save a
`scores.csv` file for further analysis. A readyâ€‘toâ€‘run Colab notebook is also
provided as `colab_meta_agentic_tree_search.ipynb`.

### Offline setup
When installing without network access, first build a wheelhouse on a
machine with connectivity:

```bash
mkdir -p /media/wheels
pip wheel -r requirements.txt -w /media/wheels
```

Copy `/media/wheels` to the offline machine and set `WHEELHOUSE` so
`pip` installs from this directory:

```bash
WHEELHOUSE=/media/wheels pip install -r requirements.txt
```

The repository's setup script automatically uses a `wheels/` directory
in the project root when present, so placing your pre-built wheels
there also works.

### Environment variables
The demo consults a few environment variables when choosing a rewrite strategy
and model. Set these if you do not pass ``--rewriter`` or ``--model`` on the
command line:

- ``MATS_REWRITER`` â€“ forces the rewrite engine to ``random``, ``openai`` or
  ``anthropic``.
- ``OPENAI_MODEL`` â€“ default model used by the OpenAI rewriter and bridge
  (defaults to ``gpt-4o``).
- ``ANTHROPIC_MODEL`` â€“ model name for the Anthropic rewriter
  (defaults to ``claude-3-opus-20240229``).

If ``MATS_REWRITER`` is unset the script picks ``openai`` when an
``OPENAI_API_KEY`` is present or ``anthropic`` when ``ANTHROPIC_API_KEY`` is
configured, falling back to the offline rewriter otherwise.

### Notebook quick start
1. Click the â€œOpen In Colabâ€ badge at the top of this document.
2. Execute the first cell to clone the repository and install dependencies.
3. Optionally provide `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` values in the second cell.
4. Run the demo cell to launch the search loop.
5. Optionally invoke `openai_agents_bridge.py --verify-env` from a new cell to confirm your runtime.

Add ``--enable-adk`` to the command above to start the optional ADK
gateway for remote control via the A2A protocol.
The default environment is a simple numberâ€‘line task defined in `mats/env.py` where each agent must approach a target integer. Pass `--target 7` (for example) to experiment with different goals.
Use `--seed 42` to reproduce a specific search trajectory.

> **Tip:** Replay real tick data with:
> `python run_demo.py --market-data my_feed.csv`

## 6â€‚Repository layout
```
meta_agentic_tree_search_v0/
â”œâ”€â”€ README.md                â† you are here
â”œâ”€â”€ run_demo.py              â† entryâ€‘point wrapper
â”œâ”€â”€ mats/                    â† core library
â”‚   â”œâ”€â”€ tree.py
â”‚   â”œâ”€â”€ meta_rewrite.py
â”‚   â”œâ”€â”€ evaluators.py
â”‚   â””â”€â”€ env.py
â””â”€â”€ configs/
    â””â”€â”€ default.yaml
```

## 7â€‚Walkâ€‘through of the demo episode
1. Bootstrap 4 vanilla PPO agents trading a synthetic GPUâ€‘demand proxy.  
2. Tree search explores ~300 rewrite paths within the 30â€‘second budget.  
3. Best leaf realises a 3.1â€¯% IRR over a 10â€‘day horizon (toy setting).  
4. Log files + tensorboard summaries land in `./logs/`.

## 8â€‚Extending this prototype
| Goal                           | Hook/function                     |
|--------------------------------|-----------------------------------|
| Plugâ€‘in real execution broker  | `mats.env.LiveBrokerEnv`          |
| Swap rewrite strategy          | Subclass `MetaRewriter`           |
| Use distributed workers        | `ray tune` launcher               |
| Custom tree policy             | Implement `acquire()` in `Tree`   |
| Custom output parser           | `_parse_numbers` helper           |

`LiveBrokerEnv` is a minimal subclass of :class:`NumberLineEnv` that accepts a
market data sequence. It serves as a stub for wiring real brokerage feeds into
the search loop while keeping the demo runnable completely offline.

## 9â€‚Safety & governance guardâ€‘rails
* Sandboxed codeâ€‘gen (`firejail + seccomp + tmpfs`)  
* Hard VaR budget enforced by `RiskGovernor`  
* CI tests for deterministic replay to detect edge drift  

## 10â€‚References & further reading
* **Languageâ€‘Agent Tree Search**, Jiangâ€¯etâ€¯al., ACLâ€¯2024  
* **Bestâ€‘First Agentic Tree Search**, Liâ€¯&â€¯Karim, NeurIPSâ€¯2024 workshop  
* **Selfâ€‘Referential Improvement in RL**, MÃ¼llerâ€¯etâ€¯al., arXivâ€¯2025  

## 11â€‚License
ApacheÂ 2.0 â€“ see `LICENSE`.

---
*This README belongs to the AGIâ€‘Alphaâ€‘Agent project.*
