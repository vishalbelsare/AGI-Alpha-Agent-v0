# alpha_factory_v1/demos/era_of_experience/docker-compose.experience.yml
# -----------------------------------------------------------------------
#  Era-of-Experience โข Alpha-Factory v1 ๐๏ธโจ
#  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
#  โโ PROFILES โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
#  | gpu        โ enable CUDA build & nvidia runtime                     |
#  | offline    โ start Ollama/Mixtral when OPENAI_API_KEY is unset      |
#  | observability โ Prometheus + Grafana stack                          |
#  | live-feed  โ attach PostgreSQL TimescaleDB stream logger            |
#  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
#
#  Inspired by OpenAI Agents SDK ยง6 deployment patterns (2025-04).
# -----------------------------------------------------------------------
version: "3.9"

x-common-env: &common-env
  TZ: UTC
  PYTHONUNBUFFERED: "1"
  # leave blank for full offline mode
  OPENAI_API_KEY: "${OPENAI_API_KEY:-}"
  MODEL_NAME: "${MODEL_NAME:-gpt-4o-mini}"
  TEMPERATURE: "${TEMPERATURE:-0.2}"
  # โ live metrics (optional)
  PROM_PUSHGATEWAY: "http://prom-push:9091"

x-health-curl: &hc
  interval: 20s
  retries: 5
  start_period: 30s
  test: ["CMD-SHELL", "curl -f $${HEALTH_URL} || exit 1"]

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโ SERVICES โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
services:
  # โโโโโโโโโโโโโ Vector Memory (Qdrant) โโโโโโโโโโโโโ
  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      <<: *hc
      environment:
        HEALTH_URL: http://localhost:6333/health

  # โโโโโโโโโโโโโ TimescaleDB stream logger (live-feed) โโโโโโโโโโโโโ
  timescaledb:
    image: timescale/timescaledb-postgis:2.15.0-pg14
    profiles: ["live-feed"]
    environment:
      POSTGRES_PASSWORD: "${PG_PASSWORD:-experience}"
      POSTGRES_USER: experience
      POSTGRES_DB: exp_stream
    volumes:
      - ts_data:/var/lib/postgresql/data
    healthcheck:
      <<: *hc
      environment:
        HEALTH_URL: http://localhost:5432

  # โโโโโโโโโโโโโ Redis queue for async tools โโโโโโโโโโโโโ
  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]

  # โโโโโโโโโโโโโ Offline LLM fallback (Mixtral) โโโโโโโโโโโโโ
  ollama:
    image: ollama/ollama:latest
    profiles: ["offline"]
    tty: true
    environment:
      - OLLAMA_MODELS=mixtral:instruct
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      <<: *hc
      environment:
        HEALTH_URL: http://localhost:11434/.well-known/ready

  # โโโโโโโโโโโโโ Orchestrator Agent โโโโโโโโโโโโโ
  orchestrator:
    build:
      context: ../..
      dockerfile: ./Dockerfile
      args:
        ENABLE_CUDA: "${ENABLE_CUDA:-0}"   # toggled by --profile gpu
    image: alpha_factory_orchestrator:experience
    command: python /app/demo/agent_experience_entrypoint.py
    env_file: ./config.env
    environment:
      <<: *common-env
      VECTOR_DB_URL: "http://qdrant:6333"
      REDIS_URL: "redis://redis:6379/0"
      DATABASE_URL: "postgresql://experience:${PG_PASSWORD:-experience}@timescaledb:5432/exp_stream"
      LLM_BASE_URL: "${LLM_BASE_URL:-http://ollama:11434/v1}"
    volumes:
      - ./:/app/demo:ro
    depends_on:
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
      timescaledb:
        condition: service_healthy
    ports:
      - "7860:7860"
    healthcheck:
      <<: *hc
      environment:
        HEALTH_URL: http://localhost:7860/__live
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: all
          cpus: "0.50"
          memory: 1G

  # โโโโโโโโโโโโโ Observability stack (optional) โโโโโโโโโโโโโ
  prom-push:
    image: prom/pushgateway:latest
    profiles: ["observability"]
    ports:
      - "9091:9091"

  prometheus:
    image: prom/prometheus:latest
    profiles: ["observability"]
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command: ["--config.file=/etc/prometheus/prometheus.yml"]
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana-oss:11.0.0
    profiles: ["observability"]
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=experience
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana_dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./observability/grafana_datasources:/etc/grafana/provisioning/datasources:ro
    ports:
      - "3001:3000"
    depends_on:
      - prometheus

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโ VOLUMES โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
volumes:
  qdrant_data:
  ts_data:
  redis_data:
  ollama_models:
  grafana_data:
