"""
efficiency_reward.py â€“ Alphaâ€‘FactoryÂ v1 ðŸ‘ï¸âœ¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reward backend that **encourages computational *and* economic efficiency**
while keeping the interface deadâ€‘simple: just drop a lightweight dict in the
`result` field of a toolâ€‘call and the orchestrator gets back a scalar reward in
the closed unit intervalÂ [0,â€¯1].

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Example payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    result = {
        # latency of the *overall* action perceived by the user (ms)
        "latency_ms" : 820,
        # prompt + completion tokens for LLM calls (or total tokens for batch)
        "tokens"     : 125,
        # marginal direct cost (USD) â€“â€†model call, API, eâ€‘gress, etc.
        "cost_usd"   : 0.0014,
        # measured or estimated energy consumption (joules)
        "energy_j"   : 22.3,
        # taskâ€‘specific value/utility forecast inÂ [0,â€¯1] (optional, see below)
        "value"      : 0.67,
    }

If the producer cannot measure some fields *just omit them* â€“ sensible
defaults, tuned for the AGIâ€‘Alpha demo infra, kick in.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scoring heuristic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. **Normalise** each â€œcost dimensionâ€ against an *aspirational baseline*
   (configurable via envâ€‘vars; see BASELINES below).

2. **Aggregate** the normalised penalties with a weighted sumÂ Â Â *P*.

       P = 0.4Â·latency + 0.3Â·tokens + 0.2Â·cost + 0.1Â·energy

3. **Blend with task value** `v âˆˆ [0,1]` coming either from the caller or
   derived upstream by the orchestrator.

       reward = v / (1 + P)              â†’   0Â â€¦Â 1

   â€¢ Missing/zero `value` â‡’ neutral rewardÂ 0.0.
   â€¢ The function is monotonic â¬‡ï¸ in penalties, monotonic â¬†ï¸ in value.
   â€¢ Asymptotes guarantee diminishing returns and bounded output.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Design notes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â–¸ Zero external deps â€“ pure PythonÂ â‰¥Â 3.9.
â–¸ Offline friendly â€“ no OSS libraries, no web calls.
â–¸ *Threadâ€‘safe readâ€‘only*; no shared mutable state.
â–¸ Baselines overrideable at runtime via envâ€‘vars, allowing infra tuning.
â–¸ Explicit `__all__` aids IDEs / static analysers.
â–¸ Lintâ€‘clean (ruff) & typed â€“ good citizens inside larger codebases.

Â©Â 2025Â Montreal.AI â€“ Apache-2.0 License
"""

from __future__ import annotations

import logging
import os
from typing import Any

__all__ = ("reward",)

_log = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ configurable baselines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _env_float(name: str, default: float) -> float:
    """Fetch *name* from env or fall back to *default* (must be >Â 0)."""
    try:
        v = float(os.getenv(name, default))
        return v if v > 0 else default
    except Exception:  # noqa: BLE001
        return default


# Baseline targets (tweak for your infra or via env)
_LAT_MS: float   = _env_float("EFF_BSLN_LAT_MS",   500.0)   # ms
_TOKENS: float   = _env_float("EFF_BSLN_TOKENS",   1500.0)  # tokens
_COST_USD: float = _env_float("EFF_BSLN_COST_USD", 0.01)    # dollars
_ENERGY_J: float = _env_float("EFF_BSLN_ENERGY_J", 50.0)    # joules

# Weights for penalty blend â€“ must sum to 1.0
_WEIGHTS = (0.4, 0.3, 0.2, 0.1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm(value: float, baseline: float) -> float:
    """Return nonâ€‘negative normalised cost component (value / baseline)."""
    if value <= 0 or baseline <= 0:
        return 0.0
    return value / baseline


def _clip01(x: float) -> float:
    """Clamp *x* to the closed [0,1] interval (handles NaNs gracefully)."""
    if not (x >= 0.0):  # catches NaN
        return 0.0
    if x > 1.0:
        return 1.0
    return x


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main entry â€‘ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reward(state: Any, action: Any, result: Any) -> float:   # noqa: D401
    """
    Efficiency reward âˆˆÂ [0,Â 1]

    Parameters
    ----------
    state   : snapshot of agent/env *ignored* by this backend
    action  : last action taken *ignored*
    result  : mapping with cost metrics (see module docstring)

    Returns
    -------
    float â€“ higher is better; 0.0 = neutral.
    """
    if not isinstance(result, dict):
        _log.debug("efficiency_reward: nonâ€‘dict result â€“ neutral reward")
        return 0.0

    try:
        value = float(result.get("value", 0.0))
    except Exception:  # noqa: BLE001
        value = 0.0

    if value <= 0.0:
        return 0.0

    try:
        latency_p = _norm(float(result.get("latency_ms", 0.0)), _LAT_MS)
        tokens_p  = _norm(float(result.get("tokens", 0.0)),     _TOKENS)
        cost_p    = _norm(float(result.get("cost_usd", 0.0)),   _COST_USD)
        energy_p  = _norm(float(result.get("energy_j", 0.0)),   _ENERGY_J)

        penalty = (
            _WEIGHTS[0] * latency_p +
            _WEIGHTS[1] * tokens_p  +
            _WEIGHTS[2] * cost_p    +
            _WEIGHTS[3] * energy_p
        )

        reward_val = value / (1.0 + penalty)
    except Exception as exc:  # noqa: BLE001
        _log.warning("efficiency_reward: error during calc â€“ %s", exc)
        return 0.0

    return _clip01(reward_val)
