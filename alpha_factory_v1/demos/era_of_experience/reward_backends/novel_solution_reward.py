
"""
novel_solution_reward.py â€“ Alphaâ€‘FactoryÂ v1 ğŸ‘ï¸âœ¨
-------------------------------------------------------------------------
Reward backend that incentivises *novel problemâ€‘solving strategies*.

Motivation
==========
In the **EraÂ ofÂ Experience** agents should *outâ€‘imagine* human priors.  We
approximate "novelty" by comparing the current *result* (any textual or
structured solution object) with a memory of past solutions:

    â€¢ If the cosine similarity < Ï„_low  â†’ **1.0**   (brandâ€‘new idea)
    â€¢ If  Ï„_lowÂ â‰¤Â simÂ <Â Ï„_high         â†’ value in (0,Â 1) by interpolation
    â€¢ Else (simÂ â‰¥Â Ï„_high)              â†’ **0.0**   (redundant)

Implementation details
----------------------
â€¢ Memory stores the *64â€‘bit SimHash* of each solution for O(1) lookup.
â€¢ Optionally, if **sentence_transformers** is importable *and* an embedding
  model path is set via the envâ€‘var ``EMBED_MODEL`` (defaults to
  ``all-MiniLM-L6-v2``) we compute embeddings for higherâ€‘fidelity cosine
  similarity.  Otherwise we fall back to the SimHash Hamming distance.
â€¢ Pureâ€‘Python, threadâ€‘safe, zero hard dependencies.

Public API (required by reward_backends framework)
--------------------------------------------------
    reward(state, action, result) -> float

Parameters
----------
state   : ignored
action  : ignored
result  : Any   â€“ current solution object (string / dict / list / â€¦)

Returns
-------
float âˆˆ [0.0,Â 1.0]

Â©Â 2025Â Montreal.AI   â€“ MITÂ License
"""

from __future__ import annotations

import os as _os
import threading as _th
import hashlib as _hl
import math as _math
from typing import Any, List, Dict

_lock = _th.Lock()

# â”€â”€ hyperâ€‘parameters (envâ€‘tunable) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_TAU_LOW  = float(_os.getenv("NOVEL_TAU_LOW", 0.25))   # novelty threshold
_TAU_HIGH = float(_os.getenv("NOVEL_TAU_HIGH", 0.75))  # redundancy threshold
_MAX_MEM  = int(_os.getenv("NOVEL_MEM_LIMIT", 2048))   # ringâ€‘buffer length

# â”€â”€ inâ€‘memory ring buffers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_sig_mem: List[int]         = []
_emb_mem: List[List[float]] | None = None
_idx = 0  # ring pointer

# â”€â”€ optional sentenceâ€‘transformers backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_have_embed = False
try:
    import importlib as _imp
    _st = _imp.import_module("sentence_transformers")
    _model_name = _os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2")
    _model = _st.SentenceTransformer(_model_name)
    _have_embed = True
except Exception:
    _have_embed = False

# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _simhash(text: str) -> int:
    """Return 64â€‘bit SimHash of *text*."""
    hv = [0]*64
    for token in text.split():
        h = int.from_bytes(_hl.sha1(token.encode()).digest()[:8], "big")
        for i in range(64):
            hv[i] += -1 if (h >> i) & 1 else 1
    bits = 0
    for i, v in enumerate(hv):
        if v < 0:
            bits |= 1 << i
    return bits


def _sim_sig(a: int, b: int) -> float:
    dist = bin(a ^ b).count("1")
    return 1.0 - dist / 64.0


def _cos(a: List[float], b: List[float]) -> float:
    dot = sum(x*y for x, y in zip(a, b))
    na  = _math.sqrt(sum(x*x for x in a)) or 1e-9
    nb  = _math.sqrt(sum(x*x for x in b)) or 1e-9
    return max(0.0, min(1.0, dot / (na*nb)))


def _to_text(obj: Any) -> str:
    if isinstance(obj, str):
        return obj
    try:
        import json as _json
        return _json.dumps(obj, sort_keys=True, ensure_ascii=False)
    except Exception:
        return repr(obj)


# â”€â”€ core API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reward(state: Any, action: Any, result: Any) -> float:  # noqa: D401
    """Compute and return novelty reward."""
    global _idx, _emb_mem

    txt = _to_text(result)
    sig = _simhash(txt)

    if _have_embed:
        emb: List[float] = _model.encode(txt, normalize_embeddings=True).tolist()

    with _lock:
        sims: List[float] = []
        for j, old_sig in enumerate(_sig_mem):
            s = _sim_sig(sig, old_sig)
            if _have_embed and _emb_mem is not None:
                s = 0.75*s + 0.25*_cos(emb, _emb_mem[j])
            sims.append(s)

        sim = max(sims) if sims else 0.0

        # update ring buffer
        if len(_sig_mem) < _MAX_MEM:
            _sig_mem.append(sig)
            if _have_embed:
                if _emb_mem is None:
                    _emb_mem = []
                _emb_mem.append(emb if _have_embed else [])
        else:
            _sig_mem[_idx] = sig
            if _have_embed and _emb_mem is not None:
                _emb_mem[_idx] = emb
            _idx = (_idx + 1) % _MAX_MEM

    # similarity â†’ reward mapping
    if sim <= _TAU_LOW:
        return 1.0
    if sim >= _TAU_HIGH:
        return 0.0
    return 1.0 - (sim - _TAU_LOW) / (_TAU_HIGH - _TAU_LOW)
