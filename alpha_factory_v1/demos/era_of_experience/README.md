<!--
Eraâ€‘ofâ€‘ExperienceÂ Demo
Alphaâ€‘FactoryÂ v1Â ğŸ‘ï¸âœ¨ â€” Multiâ€‘AgentÂ AGENTICÂ Î±â€‘AGI
Outâ€‘learn Â·Â Outâ€‘think Â·Â Outâ€‘strategise Â·Â Outâ€‘execute
Â©Â 2025Â MONTREAL.AIÂ Â Â Apacheâ€‘2.0Â License
-->

<h1 align="center">ğŸŒŒÂ EraÂ ofÂ Experience â€”Â Your lifelongâ€‘RL playground</h1>
<p align="center">
 <em>Spin up a selfâ€‘improving multiâ€‘agent spine in <strong>one command</strong>.<br>
 Watch it plan, act &amp; learn in realâ€‘time â€” on your laptop or in the cloud.</em>
</p>

> â€œAI will eclipse the limits of humanâ€‘authored data only when agents <strong>act, observe, and adapt</strong> in the world.â€ â€”Â DavidÂ Silver &amp; RichardÂ S.Â Sutton 

This demo distils that manifesto into <strong>Alphaâ€‘FactoryÂ v1</strong>. 
Within 60Â seconds you will witness an agent <em>rewrite its own playbook</em> every few turns, powered by grounded rewards, longâ€‘range memory and modelâ€‘agnostic planning â€” no dedicated GPU required.

---

## ğŸ› Â Requirements

- **DockerÂ 24+** with the Compose plugin
- At least **4Â CPUÂ cores** (or a modest GPU) for smooth local runs
- *(Optional)* `OPENAI_API_KEY` for cloud LLMsÂ â€” leave blank to use the builtâ€‘in Mixtral via Ollama
- If running without `run_experience_demo.sh`, install the
  [`openai-agents`](https://openai.github.io/openai-agents-python/) SDK and
  `gradio` via `pip install openai-agents gradio`.
  Then, you can run the script directly with a command like:
  ```bash
  SAMPLE_DATA_DIR=/path/to/csvs python agent_experience_entrypoint.py

---

## ğŸš€Â Quickâ€‘start (macOSÂ /Â WindowsÂ /Â Linux)

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/era_of_experience
python ../../../check_env.py --auto-install  # optional env check
chmod +x run_experience_demo.sh
./run_experience_demo.sh      # â† THATâ€™S IT
```

Add `--live` to pull in real sensor feeds (wearables, RSS, etc.):

```bash
./run_experience_demo.sh --live
```

1. **Docker Desktop** builds a 300â€¯MB image in â‰ˆÂ 1Â min. 
2. Your browser opens **http://localhost:7860** featuring 
  * live traceâ€‘graphÂ ğŸª„
  * reward dashboardsÂ ğŸ“ˆ
  * interactive chat / tool consoleÂ ğŸ’¬
  * builtâ€‘in alpha detectors (yield curve & supplyâ€‘chain) ğŸ”

> **Offline/Private mode** â€” leave `OPENAI_API_KEY=` blank in <code>config.env</code>; the stack falls back to <strong>OllamaÂ âœ•Â Mixtralâ€‘8x7B</strong> and stays airâ€‘gapped.

## Offline Setup

When running without internet access:

1. Pre-download `wearable_daily.csv` and `edu_progress.csv` from the
   <a href="https://github.com/MontrealAI/demo-assets">demo-assets</a> repository.
2. Place both files in `offline_samples/` before executing
   <code>./run_experience_demo.sh</code> so the orchestrator can read them.
3. If the environment check cannot reach PyPI, set `SKIP_ENV_CHECK=1` to skip
   that step:
   ```bash
   SKIP_ENV_CHECK=1 ./run_experience_demo.sh
   ```


### ğŸ”§Â Configure &amp; advanced usage

1. Copy the sample environment file and tweak as desired:

   ```bash
   cp config.env.sample config.env
   $EDITOR config.env      # set OPENAI_API_KEY, MODEL_NAME, PG_PASSWORD, LOG_LEVEL, LIVE_FEED, etc.
   ```
   You may override the path for built-in offline samples with
   `SAMPLE_DATA_DIR=/path/to/csvs`.
   Sample CSVs (`wearable_daily.csv`, `edu_progress.csv`) are shipped in
   `offline_samples/` so the demo also works without internet access.

2. Enable real-time collectors and metrics with the `--live` flag:

   ```bash
   ./run_experience_demo.sh --live
   ```

   (equivalent to setting `LIVE_FEED=1` in `config.env`)

   The orchestrator automatically switches to offline mode whenever
   `OPENAI_API_KEY` is left empty.

---

## ğŸ“Â Run on Colab (zero install)

| Notebook | Runtime | Launch |
|----------|---------|--------|
| `colab_era_of_experience.ipynb` | CPUÂ /Â GPU | <a href="https://colab.research.google.com/github/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/era_of_experience/colab_era_of_experience.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="OpenÂ inÂ Colab"></a> |

The notebook installs a lean Python stack (&lt;Â 120â€¯s), exposes Gradio via ngrok and lets you call tools directly from cells. It automatically verifies the runtime with `check_env.py` and runs the unit tests so you can confirm everything works. Example cells illustrate detecting "alpha" opportunities using the offline yield curve **and** a toy supplyâ€‘chain flow snapshot.

---

## âœ¨Â Whatâ€™s new &Â why it matters

| SilverÂ &amp;Â Suttonâ€™s pillar | How we realise it |
|---------------------------|--------------------|
| **StreamsÂ ofÂ experience** | Infinite generator feeding monthâ€‘long synthetic logs |
| **Sensorâ€‘motor actions** | Tools (`web_search`, `plan_meal`, user chat) mutate state |
| **Grounded rewards**   | Plugâ€‘ins: <code>fitness_reward</code>, <code>education_reward</code>, <code>curiosity_reward</code>, â€¦ (hotâ€‘reloaded) |
| **Nonâ€‘human reasoning**  | Monteâ€‘CarloÂ TreeÂ Search planner + vector memory â€” no CoT imitation |

Result: an agent that <strong>evolves faster than you can refresh the page</strong>.

---

## ğŸ› Â Architecture inÂ 60Â seconds

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” experience  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generator â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ OrchestratorÂ âš™ â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ toolâ€‘calls
    â–²               â–²    â–¼
 rewardâ”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚ PlannerÂ â™Ÿ â”‚ â”‚ Tools  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **OpenAIÂ AgentsÂ SDK**Â â€” composable toolâ€‘calling, function schemas, memory  
* **A2A protocol**Â â€” futureâ€‘proof multiâ€‘agent handâ€‘offs  
* **Model Context Protocol**Â â€” streaming context for huge traces  
* **Bestâ€‘practice guardrails** from OpenAI *Practical Guide to Building Agents*  

---

## ğŸ—‚Â Repo map

| Path /Â file | What it does |
|-------------|--------------|
| `agent_experience_entrypoint.py` | boots orchestrator + Gradio |
| `run_experience_demo.sh` | 1â€‘liner prod launcher (healthâ€‘gated) |
| `docker-compose.experience.yml` | orchestratorÂ + Ollama services |
| `reward_backends/` | ğŸ¬Â Dropâ€‘in reward plugâ€‘ins (autoâ€‘discovery) |
| `simulation/` | Tiny Gymâ€‘like env stubs (ready to extend) |
| `stub_agents.py` | Minimal agent classes for OpenAI SDK & ADK workflows |
| `colab_era_of_experience.ipynb` | Cloud twin notebook |
| `alpha_report.py` | CLI helper printing current offline alpha signals |

Run it with local CSVs:

```bash
python alpha_report.py --data-dir path/to/offline_samples
```

---

## ğŸ”ŒÂ Extending

* **Add a reward**

```bash
cp reward_backends/template.py reward_backends/my_reward.py
$EDITOR reward_backends/my_reward.py   # implement reward()
```

* **Register a tool**

```python
from openai_agents import Tool

@Tool(name="place_trade", description="Execute an equity order on Alpaca")
async def place_trade(ticker:str, qty:int, side:str="BUY"): ...
```

This demo ships with a minimal example:

```python
@Tool("detect_yield_curve_alpha", "Assess yield curve inversion using offline data.")
async def detect_yield_curve_alpha_tool():
    return {"alpha": detect_yield_curve_alpha()}

@Tool("detect_supply_chain_alpha", "Check for potential supply-chain disruptions using offline data.")
async def detect_supply_chain_alpha_tool(threshold: float = 50.0):
    return {"alpha": detect_supply_chain_alpha(threshold)}
```

* **Run in simulation**

The `simulation` package ships with `SimpleExperienceEnv`, a tiny
Gym-like environment for experimenting with offline loops:

```python
from alpha_factory_v1.demos.era_of_experience.simulation import SimpleExperienceEnv

env = SimpleExperienceEnv()
state = env.reset()
for _ in range(3):
    state, reward, done, info = env.step("act")
    print(state, reward)
```

* **Prototype a custom agent**

  `stub_agents.py` contains minimal classes
  (`ExperienceAgent`, `FederatedExperienceAgent`) illustrating how to build
  on the OpenAI SDK and Google ADK respectively.


* **Clusterâ€‘scale**

```bash
docker compose --profile gpu --scale orchestrator=4 up --build
```

Shared Redis memory + A2A = emergent cooperation.

---

## ğŸ›¡Â Security &Â Compliance

* Nonâ€‘root container; no Dockerâ€‘inâ€‘Docker. 
* Secrets isolated in `config.env`, never baked into images. 
* Optâ€‘in telemetry only; default is **OFF**. 
* `/__live` returns **200 OK** for K8s, Traefik, Nginx health probes. 
* <code>safety_compliance_reward.py</code> penalises violations and rewards selfâ€‘correction.

---

## ğŸ“ˆÂ Benchmarks (o3â€‘mini, 8Ã—CPUÂ vCPU)

| Metric | 1â€‘agent | 4â€‘agent swarm |
|--------|---------|---------------|
| Decisions /Â min | 38 | 124 |
| Avg reward | 0.43 | 0.57 |
| Latency P50 | 520â€¯ms | 730â€¯ms |

*(Synthetic workload; see `benchmarks/` for scripts)*

---

## âœ…Â Tests

Verify the demo locally with Python's builtin test runner:

```bash
python -m unittest tests.test_era_experience
```

Run `python ../../../check_env.py --auto-install` first to ensure optional
packages like `pytest` and `openai-agents` are available.

---

## ğŸ—ºÂ Roadâ€‘map

- [ ] Plugâ€‘andâ€‘play Gymâ€‘Retrowrapper for atariâ€‘style sims 
- [ ] Vectorâ€‘DB eviction policy learning 
- [ ] Live rewardÂ tuning UI 
- [ ] WASM build for edge devices 

---

## ğŸ“œÂ License

ApacheÂ 2.0. By using this repo you agree to cite **Montreal.AI Alphaâ€‘Factory** if you build on top.

> **Alphaâ€‘Factory** â€” forging intelligence that *outâ€‘learns, outâ€‘thinks, outâ€‘executes*.
