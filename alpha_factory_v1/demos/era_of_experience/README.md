<!--
Eraâ€‘ofâ€‘ExperienceÂ Demo
Alphaâ€‘FactoryÂ v1Â ğŸ‘ï¸âœ¨ â€” Multiâ€‘AgentÂ AGENTICÂ Î±â€‘AGI
Outâ€‘learn Â·Â Outâ€‘think Â·Â Outâ€‘strategise Â·Â Outâ€‘execute
Â©Â 2025Â MONTREAL.AIÂ Â Â MITÂ License
-->

<h1 align="center">ğŸŒŒÂ EraÂ ofÂ Experience â€”Â Your lifelongâ€‘RL playground</h1>
<p align="center">
 <em>Spin up a selfâ€‘improving multiâ€‘agent spine in <strong>one command</strong>.<br>
 Watch it plan, act &amp; learn in realâ€‘time â€” on your laptop or in the cloud.</em>
</p>

> â€œAI will eclipse the limits of humanâ€‘authored data only when agents <strong>act, observe, and adapt</strong> in the world.â€ â€”Â DavidÂ Silver &amp; RichardÂ S.Â Sutton 

This demo distils that manifesto into <strong>Alphaâ€‘FactoryÂ v1</strong>. 
Within 60Â seconds you will witness an agent <em>rewrite its own playbook</em> every few turns, powered by grounded rewards, longâ€‘range memory and modelâ€‘agnostic planning â€” no dedicated GPU required.

---

## ğŸš€Â Quickâ€‘start (macOSÂ /Â WindowsÂ /Â Linux)

```bash
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/era_of_experience
chmod +x run_experience_demo.sh
./run_experience_demo.sh      # â† THATâ€™S IT
```

Add `--live` to pull in real sensor feeds (wearables, RSS, etc.):

```bash
./run_experience_demo.sh --live
```

1. **Docker Desktop** builds a 300â€¯MB image in â‰ˆÂ 1Â min. 
2. Your browser opens **http://localhost:7860** featuring 
  * live traceâ€‘graphÂ ğŸª„ 
  * reward dashboardsÂ ğŸ“ˆ 
  * interactive chat / tool consoleÂ ğŸ’¬ 

> **Offline/Private mode** â€” leave `OPENAI_API_KEY=` blank in <code>config.env</code>; the stack falls back to <strong>OllamaÂ âœ•Â Mixtralâ€‘8x7B</strong> and stays airâ€‘gapped.

### ğŸ”§Â Configure &amp; advanced usage

1. Copy the sample environment file and tweak as desired:

   ```bash
   cp config.env.sample config.env
   $EDITOR config.env      # set OPENAI_API_KEY, MODEL_NAME, etc.
   ```

2. Enable real-time collectors and metrics with the `--live` flag:

   ```bash
   ./run_experience_demo.sh --live
   ```

   The orchestrator automatically switches to offline mode whenever
   `OPENAI_API_KEY` is left empty.

---

## ğŸ“Â Run on Colab (zero install)

| Notebook | Runtime | Launch |
|----------|---------|--------|
| `colab_era_of_experience.ipynb` | CPUÂ /Â GPU | <a href="https://colab.research.google.com/github/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/era_of_experience/colab_era_of_experience.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="OpenÂ inÂ Colab"></a> |

The notebook installs a lean Python stack (&lt;Â 120â€¯s), exposes Gradio via ngrok and lets you call tools directly from cells.

---

## âœ¨Â Whatâ€™s new &Â why it matters

| SilverÂ &amp;Â Suttonâ€™s pillar | How we realise it |
|---------------------------|--------------------|
| **StreamsÂ ofÂ experience** | Infinite generator feeding monthâ€‘long synthetic logs |
| **Sensorâ€‘motor actions** | Tools (`web_search`, `plan_meal`, user chat) mutate state |
| **Grounded rewards**   | Plugâ€‘ins: <code>fitness_reward</code>, <code>education_reward</code>, <code>curiosity_reward</code>, â€¦ (hotâ€‘reloaded) |
| **Nonâ€‘human reasoning**  | Monteâ€‘CarloÂ TreeÂ Search planner + vector memory â€” no CoT imitation |

Result: an agent that <strong>evolves faster than you can refresh the page</strong>.

---

## ğŸ› Â Architecture inÂ 60Â seconds

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” experience  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generator â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ OrchestratorÂ âš™ â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ toolâ€‘calls
    â–²               â–²    â–¼
 rewardâ”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚ PlannerÂ â™Ÿ â”‚ â”‚ Tools  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **OpenAIÂ AgentsÂ SDK**Â â€” composable toolâ€‘calling, function schemas, memory  
* **A2A protocol**Â â€” futureâ€‘proof multiâ€‘agent handâ€‘offs  
* **Model Context Protocol**Â â€” streaming context for huge traces  
* **Bestâ€‘practice guardrails** from OpenAI *Practical Guide to Building Agents*  

---

## ğŸ—‚Â Repo map

| Path /Â file | What it does |
|-------------|--------------|
| `agent_experience_entrypoint.py` | boots orchestrator + Gradio |
| `run_experience_demo.sh` | 1â€‘liner prod launcher (healthâ€‘gated) |
| `docker-compose.experience.yml` | orchestratorÂ + Ollama services |
| `reward_backends/` | ğŸ¬Â Dropâ€‘in reward plugâ€‘ins (autoâ€‘discovery) |
| `simulation/` | Tiny Gymâ€‘like env stubs (roadâ€‘map) |
| `colab_era_of_experience.ipynb` | Cloud twin notebook |

---

## ğŸ”ŒÂ Extending

* **Add a reward**

```bash
cp reward_backends/template.py reward_backends/my_reward.py
$EDITOR reward_backends/my_reward.py   # implement reward()
```

* **Register a tool**

```python
from openai_agents import Tool

@Tool(name="place_trade", description="Execute an equity order on Alpaca")
async def place_trade(ticker:str, qty:int, side:str="BUY"): ...
```

* **Clusterâ€‘scale**

```bash
docker compose --profile gpu --scale orchestrator=4 up --build
```

Shared Redis memory + A2A = emergent cooperation.

---

## ğŸ›¡Â Security &Â Compliance

* Nonâ€‘root container; no Dockerâ€‘inâ€‘Docker. 
* Secrets isolated in `config.env`, never baked into images. 
* Optâ€‘in telemetry only; default is **OFF**. 
* `/__live` returns **200 OK** for K8s, Traefik, Nginx health probes. 
* <code>safety_compliance_reward.py</code> penalises violations and rewards selfâ€‘correction.

---

## ğŸ“ˆÂ Benchmarks (o3â€‘mini, 8Ã—CPUÂ vCPU)

| Metric | 1â€‘agent | 4â€‘agent swarm |
|--------|---------|---------------|
| Decisions /Â min | 38 | 124 |
| Avg reward | 0.43 | 0.57 |
| Latency P50 | 520â€¯ms | 730â€¯ms |

*(Synthetic workload; see `benchmarks/` for scripts)*

---

## ğŸ—ºÂ Roadâ€‘map

- [ ] Plugâ€‘andâ€‘play Gymâ€‘Retrowrapper for atariâ€‘style sims 
- [ ] Vectorâ€‘DB eviction policy learning 
- [ ] Live rewardÂ tuning UI 
- [ ] WASM build for edge devices 

---

## ğŸ“œÂ License

MIT. By using this repo you agree to cite **Montreal.AI Alphaâ€‘Factory** if you build on top.

> **Alphaâ€‘Factory** â€” forging intelligence that *outâ€‘learns, outâ€‘thinks, outâ€‘executes*.
