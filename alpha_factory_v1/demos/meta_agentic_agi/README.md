
# Metaâ€‘AgenticÂ Î±â€‘AGIÂ ğŸ‘ï¸âœ¨Â Demo â€“ **Productionâ€‘Grade v0.1.0**

---

## ğŸ“ŒÂ Purpose & Positioning

This demo extends **Alphaâ€‘FactoryÂ v1** into a *selfâ€‘improving*, crossâ€‘industry â€œAlpha Factoryâ€ able to **Outâ€‘Learn Â· Outâ€‘Think Â· Outâ€‘Design Â· Outâ€‘Strategize Â· Outâ€‘Execute** â€” *without hardâ€‘wiring a single vendor or model*.

It operationalises the **Automated Designâ€¯ofâ€¯Agenticâ€¯Systems** paradigm from HuÂ *etâ€¯al.*Â ICLRâ€‘25 and layers true **multiâ€‘objective search**, openâ€‘weights support, automated lineage documentation, and antifragile safeguards on top of the existing Î±â€‘Factory.

> **Goal:** Provide a **completely deployable, auditedâ€‘byâ€‘design reference stack** that a nonâ€‘technical stakeholder can run on a laptop *or* scale up in Kubernetes, then immediately surface alphaâ€‘grade opportunities in any vertical.

---

## 1Â Quickâ€‘start ğŸ
```bash
# â¶Â Clone Alphaâ€‘Factory v1 and enter demo folder
git clone https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git
cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/meta_agentic_agi

# â·Â Create & activate env
micromamba create -n metaagi python=3.11 -y
micromamba activate metaagi
pip install -r requirements.txt          # â†“ pureâ€‘Python, CPUâ€‘only default

# â¸Â Run in ğŸ—²Â zeroâ€‘API mode (openâ€‘weights) â€“ pulls a gguf model via Ollama
python meta_agentic_agi_demo.py --provider mistral:7b-instruct.gguf

# Â â€¦or plug any provider (OpenAI, Anthropic, LMâ€‘Studioâ€‘local)
OPENAI_API_KEY=sk-â€¦ python meta_agentic_agi_demo.py --provider openai:gpt-4o

# â¹Â Launch visual lineage UI
streamlit run ui/lineage_app.py           # http://localhost:8501
```

*No GPU?Â No problem â€” default settings use dynamic lowâ€‘RAM quantisation and the search loop throttles to respect laptop thermals.*

---

## 2Â Highâ€‘Level Architecture ğŸ—
```
alpha_factory_v1/
â””â”€â”€ demos/meta_agentic_agi/
    â”œâ”€â”€ core/                       # Providerâ€‘agnostic primitives
    â”‚   â”œâ”€â”€ fm.py                  # Unified FM wrapper (OpenAI / Anthropic / llamaâ€‘cpp)
    â”‚   â”œâ”€â”€ tools.py               # Search, exec sandbox, RAG, vector store helpers
    â”‚   â””â”€â”€ prompts.py             # Shared prompt fragments (COT, Reflexion, MCP streaming)
    â”œâ”€â”€ meta_search/               # Metaâ€‘agentic search loop
    â”‚   â”œâ”€â”€ archive.py             # JSONL steppingâ€‘stone log (Pareto + novelty hashes)
    â”‚   â”œâ”€â”€ search.py              # NSGAâ€‘II evolutionary loop w/ reflexive LLM programmer
    â”‚   â””â”€â”€ scorer.py              # Accuracy â€¢ Latency â€¢ Cost â€¢ Hallucination â€¢ Carbon
    â”œâ”€â”€ agents/                    # Runtime agent interface
    â”‚   â”œâ”€â”€ agent_base.py          # forward(task:Â Info) â†’ Info | str
    â”‚   â””â”€â”€ seeds.py               # Bootstrap population (COT, Selfâ€‘Refine, etc.)
    â”œâ”€â”€ ui/
    â”‚   â”œâ”€â”€ lineage_app.py         # Streamlit dashboard â€“ graph lineage & metrics
    â”‚   â””â”€â”€ assets/                # Icons / D3 templates / Tailwind sheet
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ default.yml            # Editable inâ€‘UI â€“ objectives & provider switch
    â”œâ”€â”€ requirements.txt           # â‰¤Â 40â€¯MiB wheels; pureâ€‘py numpyâ€‘lite default
    â””â”€â”€ meta_agentic_agi_demo.py   # Entryâ€‘point CLI
```

---

## 3Â ArchitectureÂ ğŸ”
1. **Agent specÂ = Python function** â€“ *Turingâ€‘complete searchâ€‘space*.
2. **MetaÂ Agent** (GPTâ€‘4o, ClaudeÂ 3â€‘Sonnet, or local LlamaÂ 3â€‘70B) builds candidate agents **inâ€‘code**, guided by archive & multiâ€‘objective score.
3. **Evaluator** spawns sandboxed subprocesses â†’ returns metrics (accuracy ğŸ“ˆ, latency â±, cost ğŸ’°, risk ğŸ›¡).
4. **Archive** (Qualityâ€“Diversity map) retains Paretoâ€‘front & behaviouralâ€‘novelty hashes.
5. **UI** streams lineage graph (D3.js) + sparkâ€‘lines; click any node to expand full source & run.

---

## 4Â Replacing vendors â¡ï¸ openâ€‘weightsÂ ğŸ‹ï¸â€â™€ï¸
```yaml
# configs/default.yml (excerpt)
provider: mistral:7b-instruct.gguf   # any ollama / llama.cpp id
context_length: 8192
rate_limit_tps: 4
retry_backoff: 2
```
Set `provider:` to:
* `openai:gpt-4o`  â€“Â envÂ `OPENAI_API_KEY`
* `anthropic:claude-3-sonnet` â€“Â envÂ `ANTHROPIC_API_KEY`
* `mistral:7b-instruct.gguf` (default) â€“ autoâ€‘pull via **llamaâ€‘cppâ€‘python**

The wrapper normalises chat/completions and automatically chunks long contexts via **MCP** streams.

---

## 5Â True multiâ€‘objective search ğŸ¯
> **Objective vector**Â =Â `[accuracy, cost, latency, hallucinationâ€‘risk, carbon]`

* **NSGAâ€‘II** selection (fast elitist) implemented inÂ `meta_search/search.py`.
* Behaviour descriptorÂ = SHAâ€‘256 of AST of candidate agent â€” encourages divergent program shapes.
* Optional *humanâ€‘inâ€‘theâ€‘loop* override â€” thumbs up/down in UI feeds reward shaping.

---

## 6Â Security & antifragilityÂ ğŸ›¡
* Generated code executed in a **pettingâ€‘zoo**: firejailÂ + `--seccomp` + 512â€¯MiB memcg.
* Static analysis via `bandit` + dynamic taint tracking before archive promotion.
* Live watchdog terminates rogue processes >Â 30â€¯s CPU.

---

## 7Â ExtendingÂ ğŸ› 
1. **Add dataset** â†’ dropÂ `my_dataset.pkl` into `data/` and point CLI to it.
2. **Custom metric** â†’ subclass `scorer.BaseMetric` & list in `configs/default.yml`.
3. **New tool** â†’ write `core/tools/my_tool.py`, expose `__call__`, import in seeds.

---

## 8Â RoadmapÂ ğŸ—º
* â˜Â Hierarchical Metaâ€‘Meta search (selfâ€‘improving metaâ€‘agent)
* â˜Â CUDA batch inference kernel (Flashâ€‘infer)
* â˜Â Offline RL searchâ€‘policy fineâ€‘tuning via lineage replay

---

## 9Â ReferencesÂ ğŸ“š
* Huâ€¯*etâ€¯al.* â€œAutomated Design of Agentic Systemsâ€ ICLRâ€¯2025
* OpenAI â€œA Practical Guide to Building Agentsâ€ (2024)
* Google ADK docs (2025)

---

Â©â€¯2025â€¯MONTREAL.AI â€“Â Apacheâ€‘2.0
