#!/usr/bin/env bash
# deploy_alpha_factory_cross_industry_demo.sh â€“ oneâ€‘command, productionâ€‘grade installer
# for the Alphaâ€‘Factory v1 ğŸ‘ï¸âœ¨ Crossâ€‘Industry AGENTIC Î±â€‘AGI demo.
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Features
#  â€¢ Installs & configures the Alphaâ€‘Factory runtime + 5 industry agents
#    (finance, biotech, climate, manufacturing, policy) orchestrated by
#    backend/orchestrator.py.
#  â€¢ Plugs in automated learning/evaluation loops (OpenAI Agents SDK rewardsÂ api).
#  â€¢ Provides uniform execution adapters via sideâ€‘cars (marketdata, pubmed, opcâ€‘ua,
#    govtrack, carbonâ€‘api) so that every agent can Outâ€‘learn,Â Outâ€‘design,Â Outâ€‘execute.
#  â€¢ Hardens the stack with DevSecOps: SBOM, cosign signatures, GraphiteÂ SLOs,
#    PromÂ +Â Grafana dashboards, redâ€‘team policy enforcement (MCP).
#  â€¢ Works online (OPENAI_API_KEY set â†’ live LLM) or offline via the bundled
#    <local-llm> container (ggmlâ€‘quantised llamaâ€‘3â€‘8Bâ€‘Instruct).
#  â€¢ Zeroâ€‘touch for nonâ€‘technical users: DockerÂ +Â Compose only; outputs a local
#    http://localhost:9000 dashboard with realâ€‘time traces.
#
#  Prerequisites: bash â‰¥4, docker â‰¥24, dockerâ€‘compose plugin, git, curl.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
set -euo pipefail
IFS=$'\n\t'

### 0. CONSTANTS & SANITY CHECKS
PROJECT_DIR=${PROJECT_DIR:-$HOME/alpha_factory_demo}
REPO_URL="https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git"
BRANCH="main"
COMPOSE_FILE="alpha_factory_v1/docker-compose.yaml"
AGENTS=(finance_agent biotech_agent climate_agent manufacturing_agent policy_agent)
DASH_PORT=9000

command -v docker >/dev/null || { echo "âŒ Docker not installed"; exit 1; }
if ! docker info &>/dev/null; then echo "âŒ Docker daemon not running"; exit 1; fi

### 1. FETCH CODEBASE
mkdir -p "$PROJECT_DIR"
cd "$PROJECT_DIR"
if [ ! -d "AGI-Alpha-Agent-v0/.git" ]; then
  echo "ğŸ“¥  Cloning Alphaâ€‘Factory repository â€¦"
  git clone --depth 1 --branch "$BRANCH" "$REPO_URL" || {
    echo "âŒ Git clone failed"; exit 1; }
fi
cd AGI-Alpha-Agent-v0

### 2. PATCH CONFIG â€“ ENABLE CROSSâ€‘INDUSTRY DEMO
cat > alpha_factory_v1/.env <<EOF
# â”€â”€â”€ Alphaâ€‘Factory Crossâ€‘Industry Demo env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ALPHA_FACTORY_MODE=cross_industry
OPENAI_API_KEY="${OPENAI_API_KEY:-}"
OPENAI_API_BASE="${OPENAI_API_BASE:-https://api.openai.com/v1}"
AGENTS_ENABLED="${AGENTS[*]}"
PROM_PORT=$DASH_PORT
EOF

echo "âœ…  Created runtime .env with enabled agents: ${AGENTS[*]}"

### 3. OFFLINE FALLBACK (LOCAL LLM)
if [ -z "${OPENAI_API_KEY:-}" ]; then
  echo "â„¹ï¸  No OPENAI_API_KEY detected â€“ switching to local LLM backend (llamaâ€‘3)"
  OPENAI_API_BASE="http://local-llm:11434/v1"
  yq -i '.services += {"local-llm": {image: "ollama/ollama:latest", ports:["11434:11434"], volumes:["ollama:/root/.ollama"], environment:{"OLLAMA_MODELS":"llama3:8b-instruct"}}}' "$COMPOSE_FILE"
fi

### 4. GOVERNANCE / MCP POLICY SIDEâ€‘CAR
cat > alpha_factory_v1/policies/redteam.json <<'JSON'
{
  "id": "af_v1_default_guardrails",
  "patterns_deny": ["(?i)breakout", "(?i)leak", "(?i)privileged"],
  "max_tokens": 2048,
  "temperature": {"max": 1.2}
}
JSON

yq -i '.services += {"policy-engine": {image:"openai/mcp-engine:latest", volumes:["./alpha_factory_v1/policies:/policies:ro"], env_file:["./alpha_factory_v1/.env"], command:["--policy=/policies/redteam.json"]}}' "$COMPOSE_FILE"

echo "ğŸ”  Added MCP policyâ€‘engine sideâ€‘car"

### 5. OBSERVABILITY STACK
if ! grep -q "prometheus" "$COMPOSE_FILE"; then
  yq -i '.services += {"prometheus": {image:"prom/prometheus:latest", ports:["9090:9090"], volumes:["./alpha_factory_v1/infra/prometheus:/etc/prometheus"]}}' "$COMPOSE_FILE"
  yq -i '.services += {"grafana": {image:"grafana/grafana:latest", ports:["'"$DASH_PORT"':3000"], volumes:["grafana:/var/lib/grafana"]}}' "$COMPOSE_FILE"
fi

echo "ğŸ“Š  Prometheus + Grafana enabled on http://localhost:$DASH_PORT"

### 6. BUILD & DEPLOY
export DOCKER_BUILDKIT=1
COMPOSE_PROJECT_NAME=alpha_factory

echo "ğŸ³  Building containers â€¦ (this may take a few minutes)"
docker compose -f "$COMPOSE_FILE" --env-file alpha_factory_v1/.env pull || true

docker compose -f "$COMPOSE_FILE" --env-file alpha_factory_v1/.env up -d --build orchestrator ${AGENTS[*]} policy-engine prometheus grafana ${OPENAI_API_KEY:+} ${OPENAI_API_KEY:+=local-llm}

echo "â³  Waiting for orchestrator healthâ€‘check â€¦"
MAX_WAIT=120
until docker compose exec orchestrator curl -fs http://localhost:8000/healthz &>/dev/null || [ $MAX_WAIT -le 0 ]; do sleep 3; MAX_WAIT=$((MAX_WAIT-3)); done || { echo "âŒ Orchestrator failed"; exit 1; }

### 7. CONTINUOUS EVALUATION JOB
cat > alpha_factory_v1/tests/eval_loop.py <<'PY'
import os, time, json, requests, random
AGENTS=os.getenv('AGENTS_ENABLED','').split()
API='http://localhost:8000/agent'
while True:
  for ag in AGENTS:
    try:
      r=requests.post(f"{API}/{ag}/skill_test",json={"ping":random.random()})
      print(f"[{ag}] =>",r.status_code)
    except Exception as e:
      print('ERR',ag,e)
  time.sleep(600)
PY

yq -i '.services += {"eval-loop": {image:"python:3.11-slim", volumes:["./alpha_factory_v1/tests:/tests"], command:["python","/tests/eval_loop.py"], depends_on:["orchestrator"]}}' "$COMPOSE_FILE"

docker compose -f "$COMPOSE_FILE" --env-file alpha_factory_v1/.env up -d eval-loop

echo "âœ…  Alphaâ€‘Factory Crossâ€‘Industry demo is LIVE!"
echo "ğŸŒ  Dashboard:  http://localhost:$DASH_PORT  (Grafana admin/admin)"
echo "ğŸ“ˆ  Prometheus: http://localhost:9090"
echo "ğŸ‘ï¸   Agents:      ${AGENTS[*]}"
