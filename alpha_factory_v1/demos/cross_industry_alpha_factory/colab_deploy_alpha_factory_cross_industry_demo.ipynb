{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[See docs/DISCLAIMER_SNIPPET.md](../../../docs/DISCLAIMER_SNIPPET.md)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "env"
   },
   "source": "OPENAI_API_KEY = \"\"  # ‚Üê üîë  put your key here or leave empty\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Optional offline mode\n\n\n\nDownload a `.gguf` weight and set `LLAMA_MODEL_PATH` before running the setup cell. `llama-cpp-python` or `ctransformers` must be installed for local inference:\n\n\n\n```bash\n\npip install openai-agents llama-cpp-python ctransformers\n\n```\n\n\n\nExample weights and rough CPU throughput:\n\n\n\n| Model | Size | ~tokens/s |\n\n|-------|------|-----------|\n\n| TinyLlama-1.1B-Chat Q4_K_M | 380 MB | ~20 |\n\n| Llama-3-8B-Instruct Q4_K_M | 4 GB | ~5 |\n\n| Mixtral-8x7B-Instruct Q4_0 | 7 GB | ~3 |\n\n\n\nWhen `LLAMA_MODEL_PATH` is defined the orchestrator loads the weight instead of calling OpenAI.\n\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "01-setup"
   },
   "source": "%%bash\nif [ ! -d AGI-Alpha-Agent-v0 ]; then\n  git clone --depth 1 https://github.com/MontrealAI/AGI-Alpha-Agent-v0.git\nfi\ncd AGI-Alpha-Agent-v0\npip install -q -r alpha_factory_v1/requirements-colab.txt\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Offline wheelhouse install\n\nMount a Google Drive folder (or local path) containing pre-built wheels and install packages with `pip --no-index --find-links <wheelhouse>`. The definitive package list is in `alpha_factory_v1/requirements-colab.lock`.\n\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from google.colab import drive\n\ndrive.mount('/content/drive')\n\nwheelhouse = '/content/drive/MyDrive/wheelhouse'  # or any local path\n\n!pip install --no-index --find-links $wheelhouse -r AGI-Alpha-Agent-v0/alpha_factory_v1/requirements-colab.lock\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚öôÔ∏è Patch orchestrator: add `/update_model` hot-reload endpoint\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "02-patch"
   },
   "source": "%%bash\n\napply_patch() {\n\npython - <<'PY'\n\nimport importlib, textwrap, pathlib, inspect\n\nsrc = pathlib.Path('AGI-Alpha-Agent-v0/alpha_factory_v1/backend/orchestrator.py')\n\ncode = src.read_text()\n\nif 'update_model' in code:\n\n    print('‚úÖ update_model already present'); raise SystemExit\n\ninsertion = textwrap.dedent('''\\n@router.post(\"/agent/{agent_id}/update_model\")\\nasync def update_model(agent_id: str, file: bytes = File(...)):\\n    if agent_id not in AGENT_REGISTRY:\\n        raise HTTPException(status_code=404, detail=\"agent not found\")\\n    import tempfile, zipfile, io\\n    with tempfile.TemporaryDirectory() as td:\\n        zf = zipfile.ZipFile(io.BytesIO(file)); zf.extractall(td)\\n        AGENT_REGISTRY[agent_id].load_weights(td)\\n    return {\"status\":\"ok\"}\\n''')\n\nmarker = '# === ROUTES ==='\n\nidx = code.index(marker) + len(marker)\n\nnew_code = code[:idx] + insertion + code[idx:]\n\nsrc.write_text(new_code)\n\nprint('üöÄ patched orchestrator with hot-reload endpoint')\n\nPY\n\n}\n\napply_patch\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ Launch orchestrator, agents, Ray & mock adapters (background)\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "03-launch"
   },
   "source": "%%bash --bg\n\ncd AGI-Alpha-Agent-v0/alpha_factory_v1\n\nray start --head --dashboard-host 0.0.0.0 --port 6379 --dashboard-port 8265 &>/dev/null &\n\nuvicorn backend.orchestrator:app --host 0.0.0.0 --port 8000 &>/dev/null &\n\n\n\n# tiny PubMed & Carbon adapters\n\npython - <<'PY' &>/dev/null &\n\nfrom fastapi import FastAPI; import uvicorn, random\n\napp=FastAPI(); @app.get('/')\n\ndef root(): return {\"msg\":\"ok\"}\n\nuvicorn.run(app,host='0.0.0.0',port=8005)\n\nPY\n\npython - <<'PY' &>/dev/null &\n\nfrom fastapi import FastAPI; import uvicorn\n\napp=FastAPI(); @app.get('/co2')\n\ndef co2(): return {\"ppm\":420.42}\n\nuvicorn.run(app,host='0.0.0.0',port=8010)\n\nPY\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîó Expose Grafana & Prometheus with `pyngrok`\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "04-ngrok"
   },
   "source": "from pyngrok import ngrok, conf\n\nconf.get_default().region = 'us'\n\nimport os\n\nport = int(os.getenv('DASH_PORT', 9000))\n\ngrafana = ngrok.connect(port, 'http'); prom = ngrok.connect(9090, 'http')\n\nprint('Grafana  ‚Üí', grafana.public_url)\n\nprint('Prometheus ‚Üí', prom.public_url)\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üèãÔ∏è Quick k6 load-test (20 VUs √ó 30 s)\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05-k6"
   },
   "source": "%%bash\nset -euo pipefail\n\ncat > k6.js <<'JS'\n\nimport http from 'k6/http'; import {sleep} from 'k6';\n\nexport let options = {vus:20,duration:'30s'};\n\nconst A=['finance_agent','biotech_agent','climate_agent','manufacturing_agent','policy_agent'];\n\nexport default function(){\n\n  const a=A[Math.floor(Math.random()*A.length)];\n\n  http.post(`http://127.0.0.1:8000/agent/${a}/skill_test`,JSON.stringify({ping:Math.random()}),{headers:{'Content-Type':'application/json'}});\n\n  sleep(0.05);\n\n}\n\nJS\n\nk6 run k6.js\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚ôªÔ∏è Continuous PPO trainer (runs async)\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "06-trainer"
   },
   "source": "%%bash --bg\n\npython AGI-Alpha-Agent-v0/alpha_factory_v1/continual/ppo_trainer.py\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚úÖ Smoke-probe orchestrator + sample agent call\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "07-health"
   },
   "source": "import requests, time, pprint, json, os\n\nfor _ in range(20):\n\n  try:\n\n    if requests.get('http://127.0.0.1:8000/healthz').status_code==200:\n\n      print('orchestrator healthy'); break\n\n  except: pass; time.sleep(2)\n\nresp=requests.post('http://127.0.0.1:8000/agent/finance_agent/skill_test',json={'ping':123}).json()\n\npprint.pp(resp)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n### üéâ All set!\n\n\n\n* Use the **Grafana** link above (`admin / admin`) to explore dashboards (import JSON if blank)\n\n* Rewards tune automatically; edit `rubric.json` in `continual/` and rerun the trainer cell\n\n* Adapt this notebook to plug in your own domain adapters or extra agents\n\n* When finished, run `docker compose down -v` (or `./teardown_alpha_factory_cross_industry_demo.sh`) to clean up\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Troubleshooting\n\nThe demo typically completes setup in under 10 minutes. A code cell prints the detected GPU via `nvidia-smi -L` and falls back to the CPU when no GPU is present.\nLong waits often indicate missing GPU acceleration or network stalls.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
