# SPDX-License-Identifier: Apache-2.0
"""
alpha_factory_v1.backend.memory_fabric
======================================

­ЪДа  MEMORY FABRIC  (v1.3.2 РђЊ 2025-05-02)
----------------------------------------
Production-grade episodic & causal memory layer for Alpha-Factory v1.

Highlights
РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
Рђб **Vector Memory**   РђЊ PostgreSQL + pgvector РќИ FAISS РќИ SQLite fallback (CPU-only)
Рђб **Graph Memory**    РђЊ  Neo4j РќИ NetworkX РќИ python-list fallback
Рђб **Sync & Async**    РђЊ one call signature, fabric switches under the hood
Рђб **Metrics & Tracing** РђЊ Prometheus + OpenTelemetry (graceful if libs absent)
Рђб **Graceful-Degrade** РђЊ never throws un-caught exceptions; embeddings fall back
  to SBERT or hashing and always return data
Рђб **Thread/Task safe** РђЊ re-entrant locks + asyncio.Lock for mixed usage
Рђб **No secrets in code** РђЊ all configuration via env-vars or pydantic settings
Рђб **Self-Provisioning** РђЊ creates tables, indices, constraints on first use
Рђб **One-command export** РђЊ `mem.export_all("snapshot.parquet")`
Рђб **Graceful shutdown** РђЊ use `with MemoryFabric()` or call `mem.close()` to
  release DB connections

Environment variables (factory defaults in brackets)
РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
PGHOST / PGPORT[5432] / PGUSER / PGPASSWORD / PGDATABASE[memdb]
PGVECTOR_INDEX_IVFFLAT_LISTS[100]  РђЊ performance tuning
NEO4J_URI[bolt://localhost:7687] / NEO4J_USER[neo4j] /
NEO4J_PASSWORD[neo4j] (NEO4J_PASS deprecated)
OPENAI_API_KEY (optional) РђЊ OpenAI embeddings with SBERT/hashing fallback
VECTOR_DIM[768]           РђЊ embedding dimension for pgvector & FAISS
MEM_TTL_SECONDS[0]        РђЊ 0 = keep forever, else soft-delete after TTL
MEM_MAX_PER_AGENT[100000] РђЊ per-agent quota (oldest evicted on overflow)
VECTOR_SQLITE_PATH[vector_mem.db] РђЊ file path for SQLite fallback

Python extras automatically used when available:
    numpy, sentence_transformers, psycopg2-binary, faiss-cpu, neo4j,
    networkx, prometheus_client, opentelemetry-api / sdk
"""

from __future__ import annotations

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ stdlib РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
import asyncio
import contextlib
import hashlib
import json
import logging
import math
import os
import sqlite3
import threading
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union, Final

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ dynamic soft-deps РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
try:
    import numpy as np  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - optional dep
    np = None  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    from sentence_transformers import SentenceTransformer  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    import psycopg2  # type: ignore
    import psycopg2.extras  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    import faiss  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    from neo4j import GraphDatabase  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    import networkx as nx  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    from prometheus_client import Counter, Histogram  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    from opentelemetry import trace  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    import openai  # type: ignore
with contextlib.suppress(ModuleNotFoundError):
    try:
        from pydantic import BaseSettings, Field, PositiveInt  # type: ignore
    except ImportError:  # pragma: no cover - pydantic >= 2
        from pydantic import Field, PositiveInt  # type: ignore
        from pydantic_settings import BaseSettings  # type: ignore

if "BaseSettings" not in globals():  # pragma: no cover - fallback when missing

    class BaseSettings:  # type: ignore
        """Minimal stub mimicking pydantic BaseSettings."""

        def __init__(self) -> None:  # noqa: D401 - simple stub
            for name, default in self.__class__.__dict__.items():
                if name.startswith("_") or name == "Config" or callable(default):
                    continue
                value = os.getenv(name, default)
                if isinstance(default, bool):
                    value = str(value).lower() in {"1", "true", "yes", "on"}
                elif isinstance(default, int) and default is not None:
                    try:
                        value = int(value)
                    except Exception:
                        value = default
                self.__dict__[name] = value

    def Field(default: Any, **_: Any) -> Any:  # type: ignore
        return default

    PositiveInt = int  # type: ignore

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ logging РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
logger = logging.getLogger("AlphaFactory.MemoryFabric")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    _h = logging.StreamHandler()
    _h.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s | %(message)s"))
    logger.addHandler(_h)


# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ configuration (pydantic) РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
class _Settings(BaseSettings):
    # Vector
    PGHOST: Optional[str] = None
    PGPORT: int = 5432
    PGUSER: Optional[str] = None
    PGPASSWORD: Optional[str] = None
    PGDATABASE: str = "memdb"
    PGVECTOR_INDEX_IVFFLAT_LISTS: int = 100
    VECTOR_DIM: int = 768

    # Graph
    NEO4J_URI: str = "bolt://localhost:7687"
    NEO4J_USER: str = "neo4j"
    NEO4J_PASSWORD: str = "neo4j"
    NEO4J_PASS: Optional[str] = None  # deprecated fallback

    # Memory policies
    MEM_TTL_SECONDS: int = 0  # 0 = infinite
    MEM_MAX_PER_AGENT: PositiveInt = Field(100_000, env="MEM_MAX_PER_AGENT")

    # Quotas / circuit breaker
    MEM_FAIL_GRACE_SEC: int = 20

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"


CFG = _Settings()  # single instance
# Backward compatibility: allow deprecated NEO4J_PASS
if CFG.NEO4J_PASSWORD == "neo4j" and (os.getenv("NEO4J_PASS") or CFG.NEO4J_PASS):
    CFG.NEO4J_PASSWORD = os.getenv("NEO4J_PASS", CFG.NEO4J_PASS or "neo4j")

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ telemetry helpers РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
if "Counter" in globals():
    from alpha_factory_v1.backend.metrics_registry import get_metric as _reg_metric

    def _get_metric(cls, name: str, desc: str):
        return _reg_metric(cls, name, desc)

    _MET_V_ADD = _get_metric(Counter, "af_mem_vector_add_total", "Vectors stored")
    _MET_V_SRCH = _get_metric(
        Histogram,
        "af_mem_vector_search_latency_seconds",
        "Vector search latency",
    )
else:
    _MET_V_ADD = None
    _MET_V_SRCH = contextlib.nullcontext()

tracer = trace.get_tracer(__name__) if "trace" in globals() else None  # type: ignore

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ EMBEDDING back-end РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ


def _load_embedder() -> Callable[[str], Sequence[float]]:
    """Return an embedding function with automatic fallback."""

    def _hash(text: str, dim: int = CFG.VECTOR_DIM) -> Sequence[float]:
        h = hashlib.sha256(text.encode()).digest()
        v = [(1 if b & 1 else -1) * ((b >> 1) / 128.0) for b in h]
        v *= (dim + len(v) - 1) // len(v)
        if np is not None:
            vec = np.array(v[:dim], dtype="float32")
            vec /= np.linalg.norm(vec) or 1
            return vec.tolist()
        vec = v[:dim]
        norm = math.sqrt(sum(x * x for x in vec)) or 1.0
        return [x / norm for x in vec]

    _fallback = _hash
    if np is not None and "SentenceTransformer" in globals():
        logger.info("MemoryFabric: using local SBERT embeddings.")
        _model = SentenceTransformer("all-MiniLM-L6-v2")

        def _sbert(text: str) -> Sequence[float]:
            return _model.encode(text, normalize_embeddings=True)

        _fallback = _sbert
    else:
        logger.warning("MemoryFabric: no embedding backend Рєњ hashing fallback.")

    if "openai" in globals() and os.getenv("OPENAI_API_KEY"):
        logger.info("MemoryFabric: using OpenAI embeddings with local fallback.")
        model = "text-embedding-3-small"

        def _openai(text: str) -> Sequence[float]:
            try:
                resp = openai.Embedding.create(model=model, input=text)  # type: ignore[attr-defined]
                return resp["data"][0]["embedding"]
            except (openai.OpenAIError, OSError) as exc:  # type: ignore[attr-defined]
                logger.warning("OpenAI embedding failed: %s РђЊ falling back to local embedder", exc)
                return _fallback(text)

        return _openai

    return _fallback


_EMBED = _load_embedder()

# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ util РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
_NOW: Callable[[], datetime] = lambda: datetime.now(timezone.utc)  # noqa: E731


def _hash_content(s: str) -> str:
    return hashlib.sha1(s.encode()).hexdigest()


# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ VECTOR STORE РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
class _VectorStore:
    """Implementation chain:  Postgres+pgvector РќИ FAISS РќИ SQLite РќИ RAM list."""

    # РћђРћђРћђРћђРћђРћђРћђРћђРћђ init РћђРћђРћђРћђРћђРћђРћђРћђРћђ
    def __init__(self) -> None:
        self._lock = threading.RLock()
        self._alock = asyncio.Lock()
        self._mode = "ram"
        self._fail_until = 0.0
        self._pg: Optional[psycopg2.extensions.connection] = None
        self._sql: Optional[sqlite3.Connection] = None
        self._init_postgres()  # may set self._mode
        if self._mode == "ram":
            self._init_faiss_or_sqlite()

    # РћђРћђРћђРћђРћђ Postgres / pgvector РћђРћђРћђРћђРћђ
    def _init_postgres(self) -> None:
        if "psycopg2" not in globals() or not CFG.PGHOST:
            return
        dsn = {
            "host": CFG.PGHOST,
            "port": CFG.PGPORT,
            "user": CFG.PGUSER,
            "password": CFG.PGPASSWORD,
            "dbname": CFG.PGDATABASE,
        }
        try:
            self._pg = psycopg2.connect(**{k: v for k, v in dsn.items() if v is not None})  # type: ignore[arg-type]
            with self._pg, self._pg.cursor() as cur:
                cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                cur.execute(
                    f"""CREATE TABLE IF NOT EXISTS memories (
                           id          BIGSERIAL PRIMARY KEY,
                           agent       TEXT,
                           hash        CHAR(40) UNIQUE,
                           embedding   VECTOR({CFG.VECTOR_DIM}),
                           content     TEXT,
                           ts          TIMESTAMPTZ DEFAULT NOW()
                       );"""
                )
                cur.execute("CREATE INDEX IF NOT EXISTS idx_mem_agent_ts ON memories(agent, ts DESC);")
                cur.execute(
                    f"""CREATE INDEX IF NOT EXISTS idx_mem_embedding
                        ON memories USING ivfflat (embedding vector_cosine_ops)
                        WITH (lists={CFG.PGVECTOR_INDEX_IVFFLAT_LISTS});"""
                )
            self._mode = "pg"
            logger.info("VectorStore: Postgres/pgvector ready.")
        except Exception as e:  # noqa: BLE001
            logger.warning("VectorStore: Postgres unavailable Рєњ %s", e)

    # РћђРћђРћђРћђРћђ FAISS / SQLite fallback РћђРћђРћђРћђРћђ
    def _init_faiss_or_sqlite(self) -> None:
        use_sqlite = os.getenv("VECTOR_STORE_USE_SQLITE", "false").lower() == "true"
        if np is None:
            if use_sqlite:
                logger.warning("VectorStore: numpy required for SQLite Рєњ RAM mode")
            else:
                logger.info("VectorStore: numpy missing Рєњ RAM mode")
            return
        if "faiss" in globals():
            self._faiss = faiss.IndexFlatIP(CFG.VECTOR_DIM)
            self._vectors: List[np.ndarray] = []
            self._meta: List[Tuple[str, str, str]] = []  # agent, content, ts
            self._mode = "faiss"
            logger.info("VectorStore: FAISS in-memory index ready.")
        elif use_sqlite:
            path = Path(os.getenv("VECTOR_SQLITE_PATH", "vector_mem.db"))
            self._sql = sqlite3.connect(path)
            self._sql.execute(
                (
                    "CREATE TABLE IF NOT EXISTS memories("
                    "hash TEXT PRIMARY KEY, agent TEXT, ts TEXT, "
                    "vec BLOB, content TEXT)"
                )
            )
            self._mode = "sqlite"
            logger.info("VectorStore: SQLite fallback ready.")
        else:
            logger.info("VectorStore: SQLite disabled Рєњ RAM mode")

    # РћђРћђРћђРћђРћђ internal helpers РћђРћђРћђРћђРћђ
    def _evict_if_needed(self, agent: str) -> None:
        if CFG.MEM_MAX_PER_AGENT <= 0:
            return
        if self._mode == "pg":
            with self._pg, self._pg.cursor() as cur:
                cur.execute(
                    "DELETE FROM memories WHERE id IN ("
                    "SELECT id FROM memories WHERE agent=%s ORDER BY ts ASC "
                    "OFFSET %s)",
                    (agent, CFG.MEM_MAX_PER_AGENT),
                )
        elif self._mode == "sqlite":
            cur = self._sql.execute("SELECT hash FROM memories WHERE agent=? ORDER BY ts ASC", (agent,)).fetchall()
            if len(cur) > CFG.MEM_MAX_PER_AGENT:
                to_del = cur[: len(cur) - CFG.MEM_MAX_PER_AGENT]
                self._sql.executemany("DELETE FROM memories WHERE hash=?", to_del)
                self._sql.commit()
        elif self._mode == "faiss":
            # lightweight: we simply pop oldest from meta/vectors
            while sum(1 for a, *_ in self._meta if a == agent) > CFG.MEM_MAX_PER_AGENT:
                idx = next(i for i, (a, *_) in enumerate(self._meta) if a == agent)
                self._meta.pop(idx)
                self._vectors.pop(idx)
                self._faiss.reset()
                if self._vectors:
                    self._faiss.add(np.vstack(self._vectors))

    def _apply_ttl_pg(self) -> None:
        if CFG.MEM_TTL_SECONDS <= 0:
            return
        with self._pg, self._pg.cursor() as cur:
            cur.execute("DELETE FROM memories WHERE ts < NOW() - INTERVAL '%s seconds'" % CFG.MEM_TTL_SECONDS)

    # РћђРћђРћђРћђРћђ public (sync) API РћђРћђРћђРћђРћђ
    def add(self, agent: str, content: str):
        """Insert one memory row idempotently (by SHA-1 hash)."""
        h = _hash_content(content)
        vec: Any = _EMBED(content)
        if np is not None:
            vec = np.asarray(vec, dtype="float32")
        now = _NOW().isoformat()
        try:
            if self._mode == "pg":
                if time.time() < self._fail_until:
                    raise ConnectionError("pg in grace-period")
                with self._pg, self._pg.cursor() as cur:
                    cur.execute(
                        "INSERT INTO memories(agent, hash, embedding, content) "
                        "VALUES(%s,%s,%s,%s) ON CONFLICT DO NOTHING;",
                        (agent, h, list(map(float, vec)), content),
                    )
                self._evict_if_needed(agent)
                self._apply_ttl_pg()
            elif self._mode == "faiss":
                if h in {m[1] for m in self._meta}:
                    return
                self._faiss.add(vec.reshape(1, -1))
                self._vectors.append(vec)
                self._meta.append((agent, content, now))
                self._evict_if_needed(agent)
            elif self._mode == "sqlite":
                try:
                    self._sql.execute(
                        "INSERT OR IGNORE INTO memories VALUES(?,?,?,?,?)",
                        (h, agent, now, vec.tobytes(), content),
                    )
                    self._sql.commit()
                except sqlite3.OperationalError:
                    pass
                self._evict_if_needed(agent)
            else:  # ram list
                pass
            if _MET_V_ADD:
                _MET_V_ADD.inc()
        except Exception as e:  # noqa: BLE001
            logger.error("VectorStore.add error Рєњ %s  (downgrading)", e)
            self._mode = "ram"
            self._fail_until = time.time() + CFG.MEM_FAIL_GRACE_SEC

    def add_many(self, agent: str, contents: Iterable[str]):
        for c in contents:
            self.add(agent, c)

    def recent(self, agent: str, limit: int = 20) -> List[str]:
        if self._mode == "pg":
            with self._pg.cursor() as cur:
                cur.execute(
                    "SELECT content FROM memories WHERE agent=%s " "ORDER BY ts DESC LIMIT %s",
                    (agent, limit),
                )
                return [r[0] for r in cur.fetchall()]
        if self._mode == "sqlite":
            cur = self._sql.execute(
                "SELECT content FROM memories WHERE agent=? ORDER BY ts DESC LIMIT ?",
                (agent, limit),
            )
            return [r[0] for r in cur.fetchall()]
        if self._mode == "faiss":
            return [c for a, c, *_ in reversed(self._meta) if a == agent][:limit]
        return []

    # search (single query) ------------------------------------
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        with _MET_V_SRCH.time() if _MET_V_SRCH else contextlib.nullcontext():
            qv: Any = _EMBED(query)
            if np is not None:
                qv = np.asarray(qv, dtype="float32")
            if self._mode == "pg":
                with self._pg.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:  # type: ignore[arg-type]
                    cur.execute(
                        "SELECT agent, content, ts, embedding <-> %s AS score " "FROM memories ORDER BY score LIMIT %s",
                        (list(map(float, qv)), k),
                    )
                    return cur.fetchall()
            if self._mode == "faiss" and self._vectors:
                d, idx = self._faiss.search(qv.reshape(1, -1), k)
                out = []
                for score, i in zip(d[0], idx[0]):
                    if i == -1:
                        continue
                    a, c, ts = self._meta[i]
                    out.append({"agent": a, "content": c, "ts": ts, "score": float(score)})
                return out
            if self._mode == "sqlite":
                cur = self._sql.execute("SELECT agent, vec, content, ts FROM memories")
                rows = cur.fetchall()
                scored = []
                for a, vb, c, ts in rows:
                    v = np.frombuffer(vb, dtype="float32")
                    s = float(np.dot(v, qv) / (np.linalg.norm(v) * np.linalg.norm(qv) or 1))
                    scored.append((s, a, c, ts))
                scored.sort(reverse=True)
                return [{"agent": a, "content": c, "ts": ts, "score": s} for s, a, c, ts in scored[:k]]
            return []

    # bulk search ----------------------------------------------
    def search_many(self, queries: Sequence[str], k: int = 5) -> List[List[Dict[str, Any]]]:
        return [self.search(q, k) for q in queries]

    # export / import ------------------------------------------
    def export_all(self, path: Union[str, Path]) -> None:
        """Dump entire vector store to JSON-Lines or Parquet (ext inferred)."""
        path = Path(path)
        rows = []
        if self._mode == "pg":
            with self._pg.cursor() as cur:
                cur.execute("SELECT agent, content, ts FROM memories")
                rows = cur.fetchall()
        elif self._mode == "sqlite":
            rows = self._sql.execute("SELECT agent, content, ts FROM memories").fetchall()
        elif self._mode == "faiss":
            rows = [(a, c, ts) for a, c, ts in self._meta]
        if path.suffix == ".jsonl":
            with path.open("w", encoding="utf-8") as f:
                for a, c, ts in rows:
                    f.write(json.dumps({"agent": a, "content": c, "ts": ts}) + "\n")
        else:  # parquet
            try:
                import pandas as pd  # type: ignore

                df = pd.DataFrame(rows, columns=["agent", "content", "ts"])
                df.to_parquet(path)
            except Exception as e:  # noqa: BLE001
                logger.error("export_all failed: %s", e)

    def close(self) -> None:
        """Close any open database connections."""
        if getattr(self, "_pg", None):
            try:
                self._pg.close()
            except Exception as exc:  # pragma: no cover - defensive
                logger.warning("VectorStore: Postgres close failed Рєњ %s", exc)
            finally:
                self._pg = None
        if getattr(self, "_sql", None):
            try:
                self._sql.close()
            except Exception as exc:  # pragma: no cover - defensive
                logger.warning("VectorStore: SQLite close failed Рєњ %s", exc)
            finally:
                self._sql = None


# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ GRAPH STORE РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
class _GraphStore:
    """Neo4j РќИ NetworkX РќИ list  (same downgrade policy as vector store)"""

    def __init__(self) -> None:
        self._lock = threading.RLock()
        self._alock = asyncio.Lock()
        self._mode = "list"
        self._fail_until = 0.0
        self._init_neo4j()
        if self._mode == "list":
            self._init_networkx()

    def _init_neo4j(self) -> None:
        if "GraphDatabase" not in globals():
            return
        try:
            self._driver = GraphDatabase.driver(CFG.NEO4J_URI, auth=(CFG.NEO4J_USER, CFG.NEO4J_PASSWORD))
            with self._driver.session() as s:
                s.run("RETURN 1").single()
            self._mode = "neo4j"
            logger.info("GraphStore: Neo4j ready.")
        except Exception as e:  # noqa: BLE001
            logger.warning("GraphStore: Neo4j unavailable Рєњ %s", e)

    def _init_networkx(self) -> None:
        if "nx" in globals():
            self._g = nx.MultiDiGraph()  # type: ignore[attr-defined]
            self._mode = "nx"
            logger.info("GraphStore: NetworkX in-mem fallback.")
        else:
            self._triples: List[Tuple[str, str, str, Dict[str, Any]]] = []
            logger.info("GraphStore: python-list ultimate fallback.")

    # add relation ---------------------------------------------
    def add(self, a: str, rel: str, b: str, props: Optional[Dict[str, Any]] = None) -> None:
        props = props or {}
        props.setdefault("ts", _NOW().isoformat())
        try:
            if self._mode == "neo4j":
                if time.time() < self._fail_until:
                    raise ConnectionError("neo4j in grace")
                cypher = (
                    "MERGE (x:Entity {name:$a}) "
                    "MERGE (y:Entity {name:$b}) "
                    f"MERGE (x)-[r:{rel.upper()}]->(y) "
                    "SET r += $props"
                )
                with self._driver.session() as s:
                    s.run(cypher, a=a, b=b, props=props)
            elif self._mode == "nx":
                self._g.add_edge(a, b, key=rel, **props)
            else:
                self._triples.append((a, rel, b, props))
        except Exception as e:  # noqa: BLE001
            logger.error("GraphStore.add error Рєњ %s  (downgrading)", e)
            self._mode = "list"
            self._fail_until = time.time() + CFG.MEM_FAIL_GRACE_SEC

    # query ----------------------------------------------------
    def query(self, cypher: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        if self._mode == "neo4j":
            try:
                with self._driver.session() as s:
                    if params is None:
                        return [r.data() for r in s.run(cypher)]
                    return [r.data() for r in s.run(cypher, params)]
            except Exception as e:  # noqa: BLE001
                logger.error("Neo4j query failed Рєњ %s", e)
        logger.warning("GraphStore.query: backend unavailable. returning [].")
        return []

    # find path ------------------------------------------------
    def find_path(self, start: str, end: str, max_len: int = 3) -> List[str]:
        if self._mode == "neo4j":
            q = (
                "MATCH p=(a:Entity {name:$s})-[:*1..%d]-(b:Entity {name:$e}) "
                "RETURN [n IN nodes(p) | n.name] AS names "
                "ORDER BY size(nodes(p)) ASC LIMIT 1" % max_len
            )
            res = self.query(q.replace("\n", " "), {"s": start, "e": end})
            return res[0]["names"] if res else []
        if self._mode == "nx":
            try:
                path = nx.shortest_path(self._g, start, end)  # type: ignore[arg-type]
                return path[: max_len + 1]
            except Exception:
                return []
        # list fallback: naive BFS over triples
        frontier = [(start, [start])]
        seen = {start}
        while frontier:
            node, path = frontier.pop(0)
            if len(path) > max_len + 1:
                continue
            nbrs = [c for a, _, c, _ in self._triples if a == node]
            for n in nbrs:
                if n in seen:
                    continue
                if n == end:
                    return path + [n]
                frontier.append((n, path + [n]))
                seen.add(n)
        return []

    def close(self) -> None:
        """Close any open database connections."""
        if getattr(self, "_driver", None):
            try:
                self._driver.close()
            except Exception as exc:  # pragma: no cover - defensive
                logger.warning("GraphStore: driver close failed Рєњ %s", exc)
            finally:
                self._driver = None


# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ FABRIC FACADE РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
class MemoryFabric:
    """Expose ``.vector`` and ``.graph`` stores and support ``with`` usage."""

    def __init__(self):
        self.vector = _VectorStore()
        self.graph = _GraphStore()

    # РћђРћђРћђ context manager РћђРћђРћђ
    def __enter__(self) -> "MemoryFabric":
        return self

    def __exit__(self, exc_type, exc, tb) -> bool:
        try:
            self.close()
        except Exception as err:  # pragma: no cover - defensive
            logger.warning("MemoryFabric: close failed Рєњ %s", err)
            return exc_type is None
        return False

    # РћђРћђРћђ convenience sync wrappers РћђРћђРћђ
    def add_memory(self, agent: str, content: str):
        self.vector.add(agent, content)

    def search(self, query: str, k: int = 5):
        return self.vector.search(query, k)

    def add_relation(self, a: str, rel: str, b: str, props: Optional[Dict[str, Any]] = None):
        self.graph.add(a, rel, b, props)

    def find_path(self, s: str, e: str, max_len: int = 3):
        return self.graph.find_path(s, e, max_len)

    # РћђРћђРћђ async variants (thin wrappers) РћђРћђРћђ
    async def aadd_memory(self, agent: str, content: str):
        async with self.vector._alock:
            self.vector.add(agent, content)

    async def asearch(self, query: str, k: int = 5):
        async with self.vector._alock:
            return self.vector.search(query, k)

    async def aadd_relation(self, a: str, rel: str, b: str, props: Optional[Dict[str, Any]] = None):
        async with self.graph._alock:
            self.graph.add(a, rel, b, props)

    async def afind_path(self, s: str, e: str, max_len: int = 3):
        async with self.graph._alock:
            return self.graph.find_path(s, e, max_len)

    def close(self) -> None:
        """Close vector and graph stores."""
        self.vector.close()
        self.graph.close()


# РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ global singleton РќЉРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ
mem = MemoryFabric()


def close() -> None:
    """Close the module-level ``mem`` instance."""
    mem.close()


__all__: Final[List[str]] = ["mem", "close", "MemoryFabric", "CFG"]
