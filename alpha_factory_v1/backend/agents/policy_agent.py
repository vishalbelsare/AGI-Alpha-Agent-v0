# SPDX-License-Identifier: Apache-2.0
"""backend.agents.policy_agent
===================================================================
Alphaâ€‘FactoryÂ v1 ðŸ‘ï¸âœ¨Â â€”Â Multiâ€‘AgentÂ AGENTICÂ Î±â€‘AGI
-------------------------------------------------------------------
Crossâ€‘Jurisdiction Policyâ€‘/Regâ€‘TechÂ Domainâ€‘Agent âš–ï¸ðŸ“œ (productionâ€‘grade)
===================================================================
This **PolicyAgent** ingests heterogeneous legal corpora (statutes, caseâ€‘law,
regulations, government gazettes, compliance manuals) across multiple
jurisdictions and provides retrievalâ€‘augmented answers with inline
pinâ€‘point citations, semantic risk classification (ISOÂ 37301 taxonomy) and
redâ€‘line diffs between document versions.

Key design points
-----------------
* **Experienceâ€‘first RAG**Â â€“ live RSS / API feeds of new instruments are
  streamed via Kafka topic ``pl.experience``; text is chunked, embedded and
  merged incrementally into a persistent FAISSÂ HNSW index.â€†Index metadata
  tracks jurisdiction, enactment date and version id enabling temporal
  reasoning.
* **Hybrid reasoning**Â â€“ deterministic citation retrieval (semantic ANN +
  BM25 fusion) + a MuZeroâ€‘style planner exploring followâ€‘up queries to
  resolve ambiguity and maximise answer completeness.
* **OpenAIÂ Agents SDK tools**
    â€¢ ``policy_qa``Â Â Â Â Â Â Â Â Â Â â€“ answer naturalâ€‘language queries with citations
    â€¢ ``compare_versions``Â Â â€“ semantic diff + change risk vector
    â€¢ ``risk_tag``Â Â Â Â Â Â Â Â Â Â â€“ classify snippet into ISOÂ 37301 domains
    â€¢ ``statute_search``Â Â Â Â â€“ lowâ€‘level retrieval primitive (for other agents)
* **Governance**Â â€“ every output wrapped in MCP envelope with SHAâ€‘256 digest;
  PII removed; SOX trace id logged. Prometheus gauge ``af_policy_queries``
  exports QPS.
* **Offlineâ€‘first**Â â€“ embeddings fall back to *nomicâ€‘embedâ€‘text* (SBERT)
  when OpenAI API not available; LLM answers fall back to retrieval summary.

Optional deps (lazyâ€‘loaded):
    faiss, sentence_transformers, rank_bm25, openai, kafka, httpx,
    prometheus_client, adk
"""

from __future__ import annotations

import asyncio
import difflib
import hashlib
import json
import logging
import os
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Softâ€‘optional deps (never crash at import)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import faiss  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    faiss = None  # type: ignore

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    SentenceTransformer = None  # type: ignore

try:
    from rank_bm25 import BM25Okapi  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    BM25Okapi = None  # type: ignore

try:
    import openai  # type: ignore
    from openai.agents import tool  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    openai = None  # type: ignore

    def tool(fn=None, **_):  # type: ignore
        return (lambda f: f)(fn) if fn else lambda f: f


try:
    from kafka import KafkaProducer  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    KafkaProducer = None  # type: ignore

try:
    from backend.agents import Gauge  # type: ignore
except Exception:  # pragma: no cover
    Gauge = None  # type: ignore

try:
    import adk  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    adk = None  # type: ignore
try:
    from aiohttp import ClientError as AiohttpClientError  # type: ignore
except Exception:  # pragma: no cover - optional
    AiohttpClientError = OSError  # type: ignore
try:
    from adk import ClientError as AdkClientError  # type: ignore[attr-defined]
except Exception:  # pragma: no cover - optional

    class AdkClientError(Exception):
        pass


try:
    import httpx  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    httpx = None  # type: ignore

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Alphaâ€‘Factory internals
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from backend.agent_base import AgentBase  # type: ignore
from backend.agents import AgentMetadata, register_agent  # type: ignore

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _now() -> str:
    return datetime.now(timezone.utc).isoformat()


def _digest(payload: Any) -> str:
    return hashlib.sha256(json.dumps(payload, separators=(",", ":"), sort_keys=True).encode()).hexdigest()


def _wrap_mcp(agent: str, payload: Any) -> Dict[str, Any]:
    return {
        "mcp_version": "0.2",
        "agent": agent,
        "ts": _now(),
        "digest": _digest(payload),
        "payload": payload,
    }


def _chunks(text: str, max_len: int = 512) -> List[str]:
    """Sentenceâ€‘aware chunker (â‰ˆ512 tokens)."""
    sents = re.split(r"(?<=[.!?])\s+", text)
    cur, out = [], []
    for s in sents:
        cur.append(s)
        if len(" ".join(cur)) >= max_len:
            out.append(" ".join(cur))
            cur = []
    if cur:
        out.append(" ".join(cur))
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config dataclass
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class PLConfig:
    corpus_dir: Path = Path(os.getenv("STATUTE_CORPUS_DIR", "corpus/statutes")).expanduser()
    index_path: Path = Path(os.getenv("STATUTE_INDEX_PATH", "corpus/index.faiss")).expanduser()
    embed_dim: int = int(os.getenv("PL_EMBED_DIM", 384))
    kafka_broker: Optional[str] = os.getenv("ALPHA_KAFKA_BROKER")
    exp_topic: str = os.getenv("PL_EXP_TOPIC", "pl.experience")
    openai_enabled: bool = bool(os.getenv("OPENAI_API_KEY"))
    adk_mesh: bool = bool(os.getenv("ADK_MESH"))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Embedding + retrieval
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class _Embedder:
    def __init__(self, cfg: PLConfig):
        self.dim = cfg.embed_dim
        self.use_openai = cfg.openai_enabled and openai is not None
        if not self.use_openai:
            if SentenceTransformer is None:
                raise RuntimeError("SentenceTransformer required for offline mode.")
            self._model = SentenceTransformer("nomic-embed-text")

    async def encode(self, texts: List[str]):
        import numpy as np  # local import

        if self.use_openai:
            resp = await openai.Embedding.acreate(model="text-embedding-3-small", input=texts, encoding_format="float")
            vecs = np.asarray([d.embedding for d in resp["data"]], dtype="float32")
        else:
            loop = asyncio.get_event_loop()
            vecs = await loop.run_in_executor(None, self._model.encode, texts)
            vecs = vecs.astype("float32")
        return vecs


class _Retriever:
    def __init__(self, cfg: PLConfig, embed: _Embedder):
        if faiss is None:
            raise RuntimeError("faissâ€‘cpu is required for PolicyAgent.")
        self.cfg = cfg
        self.embedder = embed
        self.index: faiss.Index = None  # type: ignore
        self.texts: List[str] = []
        self.meta: List[Dict[str, Any]] = []  # keeps jurisdiction/version
        self.bm25: Optional[BM25Okapi] = None
        asyncio.get_event_loop().run_until_complete(self._load())

    async def _load(self):
        if self.cfg.index_path.exists():
            self.index = faiss.read_index(str(self.cfg.index_path))
            data = json.loads(self.cfg.index_path.with_suffix(".json").read_text())
            self.texts, self.meta = data["texts"], data["meta"]
        else:
            await self.add_corpus(self.cfg.corpus_dir)

    async def add_corpus(self, root: Path):
        files = [p for p in root.rglob("*") if p.suffix.lower() in {".txt", ".md"}]
        new_texts, new_meta = [], []
        for p in files:
            raw = p.read_text(errors="ignore")
            juris = p.parents[0].name  # use folder name as jurisdiction tag
            ver = p.stem.split("__")[-1] if "__" in p.stem else "v1"
            for chunk in _chunks(raw):
                new_texts.append(chunk)
                new_meta.append({"file": p.name, "jurisdiction": juris, "version": ver})
        if not new_texts:
            logger.warning("No documents found in %s", root)
            return
        vecs = await self.embedder.encode(new_texts)
        faiss.normalize_L2(vecs)
        if self.index is None:
            self.index = faiss.IndexHNSWFlat(self.cfg.embed_dim, 32)
        self.index.add(vecs)
        self.texts.extend(new_texts)
        self.meta.extend(new_meta)
        # BM25 build
        if BM25Okapi is not None:
            self.bm25 = BM25Okapi([t.split() for t in self.texts])
        # persist
        self.cfg.index_path.parent.mkdir(parents=True, exist_ok=True)
        faiss.write_index(self.index, str(self.cfg.index_path))
        self.cfg.index_path.with_suffix(".json").write_text(json.dumps({"texts": self.texts, "meta": self.meta}))

    async def search(self, query: str, k: int = 6):
        vec = await self.embedder.encode([query])
        faiss.normalize_L2(vec)
        scores, idxs = self.index.search(vec, k)
        ann_hits = [(i, float(s)) for i, s in zip(idxs[0], scores[0]) if i != -1]
        # BM25 fusion
        if self.bm25 is not None:
            bm = self.bm25.get_scores(query.split())
            fused = [(i, sc + bm[i]) for i, sc in ann_hits]
            fused.sort(key=lambda x: x[1], reverse=True)
            ann_hits = fused[:k]
        return [
            {
                "text": self.texts[i],
                "score": sc,
                "meta": self.meta[i],
            }
            for i, sc in ann_hits
        ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PolicyAgent class
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PolicyAgent(AgentBase):
    NAME = "policy"
    CAPABILITIES = [
        "nl_query",
        "version_diff",
        "risk_classification",
    ]
    COMPLIANCE_TAGS = ["sox_traceable", "gdpr_minimal"]
    REQUIRES_API_KEY = False
    CYCLE_SECONDS = 3600  # passive

    def __init__(self, cfg: PLConfig | None = None):
        self.cfg = cfg or PLConfig()
        self.embedder = _Embedder(self.cfg)
        self.retriever = _Retriever(self.cfg, self.embedder)

        if self.cfg.kafka_broker and KafkaProducer:
            self._producer = KafkaProducer(
                bootstrap_servers=self.cfg.kafka_broker,
                value_serializer=lambda v: json.dumps(v).encode(),
            )
        else:
            self._producer = None

        if Gauge:
            self._qps = Gauge("af_policy_queries", "Total PolicyAgent queries")

        if self.cfg.adk_mesh and adk:
            asyncio.create_task(self._register_mesh())

    # â”€â”€ OpenAI tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @tool(description="Answer a legal / policy question with citations. Arg str query.")
    def policy_qa(self, query: str) -> str:  # noqa: D401
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(self._qa_async(query))

    @tool(description="Compare two versions. Arg JSON {'old':str,'new':str}")
    def compare_versions(self, req_json: str) -> str:  # noqa: D401
        req = json.loads(req_json)
        diff = self._diff(req.get("old", ""), req.get("new", ""))
        return json.dumps(_wrap_mcp(self.NAME, {"diff": diff}))

    @tool(description="Classify snippet into ISOÂ 37301 risk categories. Arg JSON {'text':str}")
    def risk_tag(self, req_json: str) -> str:  # noqa: D401
        text = json.loads(req_json).get("text", "")
        risks = self._classify_risk(text)
        return json.dumps(_wrap_mcp(self.NAME, {"risks": risks}))

    @tool(description="Lowâ€‘level retrieval tool. Arg JSON {'query':str,'k':int}")
    def statute_search(self, req_json: str) -> str:  # noqa: D401
        args = json.loads(req_json)
        loop = asyncio.get_event_loop()
        hits = loop.run_until_complete(self.retriever.search(args.get("query", ""), int(args.get("k", 5))))
        return json.dumps(_wrap_mcp(self.NAME, hits))

    # â”€â”€ Lifecycle (passive) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def run_cycle(self):  # noqa: D401
        # placeholder: could poll RSS feeds here
        await asyncio.sleep(self.CYCLE_SECONDS)

    async def step(self) -> None:  # noqa: D401
        """Delegate step execution to :meth:`run_cycle`."""
        await self.run_cycle()

    # â”€â”€ Internal helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _qa_async(self, query: str):
        hits = await self.retriever.search(query, 8)
        context = "\n\n".join(f"[{i+1}] {h['text']}" for i, h in enumerate(hits))
        if self.cfg.openai_enabled and openai is not None:
            chat = await openai.ChatCompletion.acreate(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a legal assistant. Answer the question using citations like [1].",
                    },
                    {"role": "user", "content": f"{query}\n\nContext:\n{context}"},
                ],
                temperature=0,
                max_tokens=700,
            )
            answer = chat.choices[0].message.content.strip()
        else:
            answer = f"(offline) Context:\n{context}"
        payload = {"answer": answer, "citations": [h["meta"] for h in hits]}
        if Gauge:
            self._qps.inc()
        if self._producer:
            self._producer.send(self.cfg.exp_topic, json.dumps({"query": query, "ts": _now()}))
        return json.dumps(_wrap_mcp(self.NAME, payload))

    def _diff(self, old: str, new: str):
        diff = difflib.unified_diff(old.splitlines(), new.splitlines(), lineterm="", fromfile="old", tofile="new")
        return "\n".join(diff)

    def _classify_risk(self, text: str):
        lowers = text.lower()
        tags = []
        if any(k in lowers for k in ("data", "personal", "subject", "processing")):
            tags.append("GDPR")
        if any(k in lowers for k in ("anti-bribery", "corruption", "fcp", "bribe")):
            tags.append("ABC")
        if any(k in lowers for k in ("antitrust", "competition", "cartel")):
            tags.append("Antitrust")
        return tags or ["unknown"]

    # â”€â”€ ADK mesh registration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _register_mesh(self):  # noqa: D401
        try:
            client = adk.Client()
            await client.register(node_type=self.NAME, metadata={"corpus": str(self.cfg.corpus_dir)})
            logger.info("[PL] registered in ADK mesh id=%s", client.node_id)
        except (AdkClientError, AiohttpClientError, asyncio.TimeoutError, OSError) as exc:
            logger.warning("ADK registration failed: %s", exc)
        except Exception as exc:  # pragma: no cover - unexpected
            logger.exception("Unexpected ADK registration error: %s", exc)
            raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Registry hook
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
register_agent(
    AgentMetadata(
        name=PolicyAgent.NAME,
        cls=PolicyAgent,
        version="0.2.0",
        capabilities=PolicyAgent.CAPABILITIES,
        compliance_tags=PolicyAgent.COMPLIANCE_TAGS,
        requires_api_key=PolicyAgent.REQUIRES_API_KEY,
    )
)

__all__ = ["PolicyAgent"]
